{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3, Indexing\n",
    "\n",
    "In this notebook you will index DBpedia (see the sub-collections listed under `https://github.com/uis-dat640-fall2019/admin/tree/master/assignments/assignment-3#data`). \n",
    "\n",
    "Make sure you specify the index settings, analyzer, and fields appropriately for to support the models to be implemented in subsequent notebooks.\n",
    "\n",
    "Note: you'll need to build a positional index. Use a single shard to make sure you're getting the right term statistics.\n",
    "\n",
    "Be sure to use both markdown cells with section headings and explanations, as well as writing readable code, to make it clear what your intention is each step of the way through the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch import helpers\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "es = Elasticsearch()\n",
    "# es.info()\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_list = stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "from IPython.display import clear_output, display, HTML\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import winsound\n",
    "duration = 500  # milliseconds\n",
    "freq = 1000  # Hz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name_term = 'test_1'\n",
    "index_name_entity = 'test_1_entities'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_SETTINGS = {\n",
    "    'settings' : {\n",
    "        'index' : {\n",
    "            \"number_of_shards\" : 1,\n",
    "            \"number_of_replicas\" : 1\n",
    "        },\n",
    "        'analysis': {\n",
    "            'analyzer': {\n",
    "                'my_english_analyzer': {\n",
    "                    'type': \"custom\",\n",
    "                    'tokenizer': \"standard\",\n",
    "                    'stopwords': \"_english_\",\n",
    "                    'filter': [\n",
    "                        \"lowercase\",\n",
    "                        \"english_stop\",\n",
    "                        \"filter_english_minimal\"\n",
    "                    ]                \n",
    "                }\n",
    "            },\n",
    "            'filter' : {\n",
    "                'filter_english_minimal' : {\n",
    "                    'type': \"stemmer\",\n",
    "                    'name': \"porter2\"\n",
    "                },\n",
    "                'english_stop': {\n",
    "                    'type': \"stop\",\n",
    "                    'stopwords': \"_english_\"\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "    },\n",
    "    'mappings': {\n",
    "        'properties': {\n",
    "            'title': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            },\n",
    "            'content': {\n",
    "                'type': \"text\",\n",
    "                'term_vector': \"with_positions\",\n",
    "                'analyzer': \"my_english_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es.indices.exists(index_name_term):\n",
    "    es.indices.delete(index=index_name_term)\n",
    "    \n",
    "es.indices.create(index=index_name_term, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if es.indices.exists(index_name_entity):\n",
    "    es.indices.delete(index=index_name_entity)\n",
    "    \n",
    "es.indices.create(index=index_name_entity, body=INDEX_SETTINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "chunk = pd.read_csv(\"data/labels_en.ttl\", sep = \" \", header = None, skiprows=1, error_bad_lines = False, warn_bad_lines = False)\n",
    "\n",
    "def process_label(label):\n",
    "    clear_output()\n",
    "    text = \" \".join(re.findall('[A-Z][^A-Z]*', label.replace(\"@en\", '')))\n",
    "    \n",
    "    label_words = text.split()\n",
    "    \n",
    "    if all([True if len(word) == 1 else False for word in label_words]) == True:\n",
    "        text = \"\".join(label_words)\n",
    "            \n",
    "    print(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "chunk = chunk.rename(columns={0: 'link', 2: 'label'})[['link', 'label']]\n",
    "print(chunk.shape)\n",
    "chunk['label'] = chunk['label'].apply(lambda label: process_label(label))\n",
    "print(chunk.shape)\n",
    "count += chunk.shape[0]\n",
    "all_data.update(chunk.set_index('link').T.to_dict())\n",
    "clear_output()\n",
    "print(chunk.shape, count, len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {}\n",
    "chunk = pd.read_csv(\"data/article_categories_en.ttl\", sep = \" \", header = None, skiprows=1, error_bad_lines = False, warn_bad_lines = False)\n",
    "\n",
    "count = 0\n",
    "\n",
    "chunk = chunk.rename(columns={0: 'link', 2: 'categories'})[['link', 'categories']]\n",
    "chunk['categories'] = chunk['categories'].apply(lambda x: x.split(\"/\")[-1].split(\":\")[-1].replace(\">\", '').replace(\"_\", \" \"))\n",
    "print(chunk.shape)\n",
    "chunk = chunk.groupby('link')['categories'].apply(list).to_frame().reset_index()\n",
    "print(chunk.shape)\n",
    "categories.update(chunk.set_index('link').T.to_dict())\n",
    "len(categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disambiguations = {}\n",
    "\n",
    "chunk = pd.read_csv(\"data/disambiguations_en.ttl\", sep = \" \", header = None, skiprows=1, error_bad_lines = False, warn_bad_lines = False)\n",
    "\n",
    "chunk = chunk.rename(columns={0: 'link', 2: 'disambiguations'})[['link', 'disambiguations']]\n",
    "chunk['link'] = chunk['link'].apply(lambda x: x.replace(\"_(disambiguation)\", ''))\n",
    "chunk['disambiguations'] = chunk['disambiguations'].apply(lambda x: x.split(\"/\")[-1].replace(\">\", '').replace(\"_\", ' ')) # add < > and underscore for entity\n",
    "print(chunk.shape)\n",
    "chunk = chunk.groupby('link')['disambiguations'].apply(list).to_frame().reset_index()\n",
    "print(chunk.shape)\n",
    "disambiguations.update(chunk.set_index('link').T.to_dict())\n",
    "\n",
    "print(len(disambiguations))\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_data = {}\n",
    "\n",
    "chunk = pd.read_csv(\"data/persondata_en.ttl\", sep = \" \", header = None, skiprows=1, error_bad_lines = False, warn_bad_lines = False)\n",
    "\n",
    "chunk = chunk.rename(columns={0: 'link', 1: 'key', 2: 'value'})[['link', 'key', 'value']]\n",
    "chunk['key'] = chunk['key'].apply(lambda key: key.split(\"/\")[-1].replace('>', '').split(\"#\")[-1])\n",
    "chunk = chunk.dropna()\n",
    "chunk['value'] = chunk['value'].apply(lambda value: value.replace('@en', '').split(\"^^\")[0].split(\"/\")[-1].replace('>', ''))\n",
    "chunk = chunk.groupby('link')\n",
    "\n",
    "for row in chunk:\n",
    "    person_data.update({ row[0]:  row[1][[\"key\", \"value\"]].T.to_dict()})\n",
    "\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text_words = []\n",
    "    for term in text.split():\n",
    "        if term not in stop_list:\n",
    "            text_words.append(porter.stem(term))\n",
    "    \n",
    "    return \" \".join(text_words)\n",
    "\n",
    "def modify_text(text):\n",
    "#     clear_output()\n",
    "#     print(text)\n",
    "    \n",
    "    try:\n",
    "        tokens = es.indices.analyze(index = index_name_term, body = {\n",
    "            'analyzer': 'my_english_analyzer',\n",
    "            'text': text.replace(\"@en\", '')\n",
    "        })['tokens']\n",
    "\n",
    "        text = \" \".join([token['token'] for token in tokens])\n",
    "        \n",
    "    except:\n",
    "        text = preprocess(text)\n",
    "        \n",
    "#     print(\"\\n-----------------------------------------------------------------------------------------------------\\n\")\n",
    "#     print(text)\n",
    "    return text\n",
    "\n",
    "# winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_abstracts = {}\n",
    "chunk_size = 1000\n",
    "chunks = pd.read_csv(\"data/long_abstracts_en.ttl\", sep = \" \", header = None, skiprows=1, chunksize = chunk_size, error_bad_lines = False, warn_bad_lines = False)\n",
    "\n",
    "i = 1\n",
    "count = 0\n",
    "for chunk in chunks:\n",
    "    clear_output()\n",
    "    print(\"Done with\", (i-1)*chunk_size)\n",
    "    print(\"Processing chunk\", i)\n",
    "    \n",
    "    chunk = chunk.rename(columns={0: 'link', 2: 'abstract'})[['link', 'abstract']]\n",
    "#     chunk['links'] = chunk['link'].apply(lambda link: [link])\n",
    "    chunk['abstract'] = chunk['abstract'].apply(lambda text: modify_text(text))\n",
    "    \n",
    "    count += chunk.shape[0]\n",
    "    chunk = chunk.set_index('link').T.to_dict()\n",
    "    long_abstracts.update(chunk)\n",
    "#     \n",
    "    i += 1\n",
    "# long_abstracts\n",
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "chunk_size = 10000\n",
    "chunks = pd.read_csv(\"data/page_links_en.ttl\", sep = \" \", header = None, skiprows=1, chunksize = chunk_size, error_bad_lines = False, warn_bad_lines = False)\n",
    "\n",
    "def add_pagelinks(row):\n",
    "    global count\n",
    "    \n",
    "    if row['link'] in all_data.keys():        \n",
    "        count += 1\n",
    "        \n",
    "        \n",
    "        break_loop = False\n",
    "        for link in row['related_links']:\n",
    "            if link in long_abstracts.keys():\n",
    "                break_loop = True\n",
    "                all_data[row['link']]['long_abstract'] = long_abstracts[link]['abstract']\n",
    "                \n",
    "            if link in person_data.keys():\n",
    "                break_loop = True\n",
    "                for i, data in person_data[link].items():\n",
    "                    all_data[row['link']][data['key']] = data['value']\n",
    "                    \n",
    "            if link in categories.keys():\n",
    "                break_loop = True\n",
    "                all_data[row['link']]['categories'] = categories[link]['categories']\n",
    "                \n",
    "            if link in disambiguations.keys():\n",
    "                break_loop = True\n",
    "                all_data[row['link']]['disambiguations'] = disambiguations[link]['disambiguations']\n",
    "            \n",
    "            if break_loop == True:\n",
    "                break\n",
    "                \n",
    "        \n",
    "        if 'related_links' in all_data[row['link']] and all_data[row['link']]['related_links'] != None:\n",
    "            all_data[row['link']]['related_links'] = all_data[row['link']]['related_links'] + [link for link in row['related_links'] if link not in all_data[row['link']]['related_links']]\n",
    "        else:\n",
    "            all_data[row['link']]['related_links'] = [row['link']] + row['related_links']\n",
    "            \n",
    "        clear_output() \n",
    "        \n",
    "        print(count, row['link'], \" | \", all_data[row['link']])\n",
    "        \n",
    "            \n",
    "for chunk in chunks:\n",
    "    chunk = chunk.rename(columns={0: 'link', 2: 'related_links'})[['link', 'related_links']]\n",
    "    chunk = chunk.groupby('link')['related_links'].apply(list).to_frame().reset_index()\n",
    "    print(chunk.shape, count)\n",
    "    chunk.apply(lambda row: add_pagelinks(row), axis = 1)\n",
    "\n",
    "# winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "winsound.Beep(freq, duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(all_data, open(\"data/index_data.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del long_abstracts\n",
    "del person_data\n",
    "del categories\n",
    "del disambiguations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "count = 0\n",
    "for key in list(all_data):\n",
    "    data = all_data[key]        \n",
    "    if 'label' not in data.keys():\n",
    "        count += 1\n",
    "        del all_data[key]\n",
    "        \n",
    "    else:\n",
    "        clear_output()\n",
    "        print(i)\n",
    "        i += 1\n",
    "        \n",
    "        all_data[i] = data\n",
    "\n",
    "        del all_data[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gendata_terms(data, index):\n",
    "    for _id, doc_ in data.items():\n",
    "        # Some preprocessing\n",
    "        clear_output()\n",
    "        \n",
    "        doc = doc_.copy()\n",
    "        \n",
    "        doc['content'] = \"\"\n",
    "            \n",
    "        if 'related_links' in doc.keys():\n",
    "            doc['content'] += \" \".join(doc['related_links'])\n",
    "        \n",
    "        if 'long_abstract' in doc.keys():\n",
    "            doc['content'] = \" \"+doc['long_abstract']\n",
    "        \n",
    "        if 'categories' in doc.keys():\n",
    "            doc['content'] += \" \"+\" \".join(doc['categories'])\n",
    "                \n",
    "        if 'disambiguations' in doc.keys():\n",
    "            doc['content'] += \" \"+\" \".join(doc['disambiguations'])\n",
    "            \n",
    "        if 'type' in doc.keys():\n",
    "            doc['content'] += \" \"+doc['type']\n",
    "\n",
    "            \n",
    "        for key in ['name', 'surname', 'givenName']:\n",
    "            if key in doc.keys():\n",
    "                doc['content'] += \" \"+doc[key]\n",
    "                \n",
    "        for key in ['birthPlace', 'deathPlace']:\n",
    "            if key in doc.keys():\n",
    "                doc['content'] += \" \"+doc[key]\n",
    "                \n",
    "        for key in ['birthDate', 'deathDate']:\n",
    "            if key in doc.keys():\n",
    "                doc['content'] += \" \"+doc[key]\n",
    "                \n",
    "        \n",
    "        \n",
    "        print(_id, doc)\n",
    "#         break\n",
    "        yield {\n",
    "            \"_index\": index,\n",
    "            \"_id\": _id,\n",
    "            \"_source\": doc,\n",
    "        }\n",
    "    \n",
    "helpers.bulk(es, gendata_terms(all_data, index_name_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gendata_entities(data, index):\n",
    "    for _id, doc_ in data.items():\n",
    "        # Some preprocessing\n",
    "        clear_output()\n",
    "        \n",
    "        doc = doc_.copy()\n",
    "        \n",
    "        if 'long_abstract' in doc.keys():\n",
    "            del doc['long_abstract']\n",
    "        \n",
    "        doc['content'] = \"\"\n",
    "        label_words = re.findall('[A-Z][^A-Z]*', doc['label'])\n",
    "        \n",
    "        if all([True if len(word) == 1 else False for word in label_words]) == False:\n",
    "            doc['label'] = \" \".join(label_words)\n",
    "            \n",
    "            \n",
    "        doc['label'] = \"<\" + doc['label'].replace(\" \", \"_\") + \">\"\n",
    "            \n",
    "        if 'related_links' in doc.keys():\n",
    "            doc['content'] += \" \".join(doc['related_links'])\n",
    "        \n",
    "        if 'categories' in doc.keys():\n",
    "            doc['categories'] = [\"<\" + category.replace(\" \", \"_\") + \">\" for category in doc['categories']]\n",
    "            doc['content'] += \" \"+\" \".join(doc['categories'])\n",
    "                \n",
    "        if 'disambiguations' in doc.keys():\n",
    "            doc['disambiguations'] = [\"<\" + disambiguation.replace(\" \", \"_\") + \">\" for disambiguation in doc['disambiguations']]\n",
    "            doc['content'] += \" \"+\" \".join(doc['disambiguations'])\n",
    "            \n",
    "        if 'type' in doc.keys():\n",
    "            doc['type'] = \"<\" + doc['type'] + \">\"\n",
    "            doc['content'] += \" \"+doc['type']\n",
    "\n",
    "            \n",
    "        for key in ['name', 'surname', 'givenName']:\n",
    "            if key in doc.keys():\n",
    "                doc[key] = \"<\"+doc[key].replace(' ', '_')+\">\"\n",
    "                doc['content'] += \" \"+doc[key]\n",
    "                \n",
    "        for key in ['birthPlace', 'deathPlace']:\n",
    "            if key in doc.keys():\n",
    "                doc[key] = \"<\"+doc[key].replace(' ', '_')+\">\"\n",
    "                doc['content'] += \" \"+doc[key]\n",
    "                \n",
    "        for key in ['birthDate', 'deathDate']:\n",
    "            if key in doc.keys():\n",
    "                doc[key] = \"<\" + doc[key] + \">\"\n",
    "                doc['content'] += \" \"+doc[key]\n",
    "                \n",
    "        \n",
    "        \n",
    "        print(_id, doc)\n",
    "#         break\n",
    "        yield {\n",
    "            \"_index\": index,\n",
    "            \"_id\": _id,\n",
    "            \"_source\": doc,\n",
    "        }\n",
    "\n",
    "helpers.bulk(es, gendata_entities(all_data, index_name_entity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
