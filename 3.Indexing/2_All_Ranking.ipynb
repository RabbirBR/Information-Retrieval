{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "stop_list=stopwords.words('english')\n",
    "porter = PorterStemmer()\n",
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "from collections import Counter\n",
    "from IPython.display import clear_output,display\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch()\n",
    "\n",
    "from pprint import pprint\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "index_term=\"test\"\n",
    "index_entity=\"test_entities\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_text(text):\n",
    "    tokens = es.indices.analyze(index = index_term, body = {\n",
    "        'analyzer': 'my_english_analyzer',\n",
    "        'text': text.replace(\"@en\", '')\n",
    "    })['tokens']\n",
    "\n",
    "    return (\" \".join([token['token'] for token in tokens]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the train queries\n",
    "\n",
    "QUERIES_FILE = \"data/queries.txt\"\n",
    "QUERIES_FILE_2 = \"data/queries2.txt\"\n",
    "\n",
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid= line.strip().split(\"\\t\")[0]\n",
    "            query=line.strip().split(\"\\t\")[1]\n",
    "            #print(qid,\"-\",query)\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "\n",
    "queries = load_queries(QUERIES_FILE_2)\n",
    "#queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepocessing the queries (stemming,stopwords)\n",
    "\n",
    "def preprocess(queries):\n",
    "    for q_id,query in queries.items():\n",
    "        query_=[]\n",
    "        for term in query.split():\n",
    "            if term not in stop_list:\n",
    "                query_.append(porter.stem(term))\n",
    "        queries[q_id]=\" \".join(query_)        \n",
    "    return queries\n",
    "\n",
    "preprocessed_q=preprocess(queries)\n",
    "#preprocessed_q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessed_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating MLM score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prob(index, term,field):\n",
    "    \"\"\"Returns the probability of the term given the field LM.\"\"\"\n",
    "    # first need to find a document that contains the term\n",
    "    hits = es.search(index=index, q = term, df = field, _source=False, size=1).get(\"hits\", {}).get(\"hits\", {})    \n",
    "    doc_id = hits[0][\"_id\"] if len(hits) > 0 else None\n",
    "    if doc_id is not None:\n",
    "        # ask for global term statistics when requesting the term vector of that doc\n",
    "        tv = es.termvectors(index=index, doc_type=\"_doc\", id=doc_id, fields=field, term_statistics=True)[\"term_vectors\"][field]\n",
    "#         print(term)\n",
    "#         pprint(tv)\n",
    "        ttf = tv[\"terms\"].get(term, {}).get(\"ttf\", 0)  # total term count in the collection (in that field)\n",
    "        sum_ttf = tv[\"field_statistics\"][\"sum_ttf\"]\n",
    "        return ttf / sum_ttf\n",
    "\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLM score for each document-query pair\n",
    "\n",
    "def get_mlm_score(query,index, doc_id, fields, weights, mu=2000):\n",
    "    clear_output()\n",
    "    FIELD_WEIGHT={}\n",
    "    for i in range(len(fields)):\n",
    "        FIELD_WEIGHT[fields[i]]=weights[i]\n",
    "      \n",
    "    tv = es.termvectors(index=index, doc_type=\"_doc\", id=doc_id, fields=fields, term_statistics=True).get('term_vectors', {})\n",
    "    print(query, list(tv.keys()),doc_id)\n",
    "\n",
    "    tf = {} # tf[field][t] holds the frequency of term `t` in a given document field; extract the values from `tv`\n",
    "    \n",
    "    score = 0  # holds log P(q|d)\n",
    "    for term in query.split():  # this is the main summation over query terms\n",
    "        ptd = 0\n",
    "        for field in fields:\n",
    "#             clear_output()\n",
    "            pttdi = 0  \n",
    "\n",
    "            if field not in tv.keys():\n",
    "                print(\"No\", field)\n",
    "                ptdi=0\n",
    "                pttdi=0\n",
    "            elif term in tv[field][\"terms\"].keys():\n",
    "#                 pprint(tv[field][\"terms\"][term])\n",
    "                \n",
    "                len_d = sum([stats['term_freq'] for term, stats in tv[field]['terms'].items()])\n",
    "                ptdi = tv[field][\"terms\"][term][\"term_freq\"] /len_d \n",
    "        \n",
    "                ptci = get_prob(index, term,field)  \n",
    "                pttdi = (ptdi + mu * ptci )/(len_d*mu) \n",
    "                \n",
    "            ptd += FIELD_WEIGHT[field] * math.log(pttdi+1) if pttdi>0 else 0\n",
    "        score += ptd\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search(indexname, query, field, size):\n",
    "    url = \"/\".join([API, indexname, \"_search\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"q\": query, \"df\": field, \"size\": size})\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)\n",
    "\n",
    "\n",
    "def rerank(preprocessed_q,fields,weights):\n",
    "    i = 0\n",
    "    mlm_scores={}\n",
    "    for q_id, query in preprocessed_q.items():\n",
    "        i+=1\n",
    "        print(\"\\n--------------------------------------------------------------------\")\n",
    "\n",
    "        print(q_id, query)\n",
    "        res = es.search(index=index_term, q=query, df=fields[0], _source=False, size = 150).get('hits', {})\n",
    "        \n",
    "        # Re-rank documents using MLM\n",
    "        scores = {}\n",
    "        for doc in res.get(\"hits\"):\n",
    "            \n",
    "            doc_id = doc.get(\"_id\")\n",
    "            scores[doc_id] = get_mlm_score(query,index_term, doc_id, fields, weights)\n",
    "\n",
    "        mlm_scores[q_id]=scores\n",
    "\n",
    "        clear_output()\n",
    "        \n",
    "    return(mlm_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlm_scores=rerank(preprocessed_q,fields=[\"label\",\"content\"],weights=[0.2,0.8])\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_scores(preprocessed_q, scores_):\n",
    "    sorted_scores = {}\n",
    "    score_df = {}\n",
    "    i = 0\n",
    "    for q_id, query in preprocessed_q.items():\n",
    "        i+=1\n",
    "        scores = scores_[q_id]\n",
    "        scores = sorted(scores.items(), key=lambda score: score[1], reverse = True)[:100]\n",
    "        sorted_scores[q_id] = scores\n",
    "\n",
    "        for score in scores:\n",
    "            clear_output()\n",
    "            score_df['QueryId'] = score_df.get('QueryId', []) + [q_id]\n",
    "\n",
    "            res = es.search(\n",
    "                index = index_term, \n",
    "                body = {\n",
    "                    'from': 0,\n",
    "                    'size': 1, # Maximum number of results to return\n",
    "                    \"query\": {\n",
    "                        \"match\": {\n",
    "                            \"_id\": score[0]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            id_ = \"<dbpedia:\"+res['hits']['hits'][0]['_source']['links'][0].split(\"/\")[-1]\n",
    "\n",
    "            print(q_id, query, score[0], id_)\n",
    "            score_df['EntityId'] = score_df.get('EntityId', []) + [id_]\n",
    "            \n",
    "    return (sorted_scores, pd.DataFrame(score_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_mlm_scores, score_df = sort_scores(preprocessed_q, mlm_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_csv(\"ranking_mlm_q2.csv\", index = False, sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating the score for unigram SDM\n",
    "\n",
    "SDM_unigram_scores=rerank(preprocessed_q,fields=[\"content\"],weights=[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the score for ordered bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def get_bigrams(preprocessed_q):\n",
    "    bigram_queries={}\n",
    "    for q_id,query in preprocessed_q.items():\n",
    "        bigarms = list(ngrams(query.split(), 2))\n",
    "        bigram_queries[q_id]=bigarms\n",
    "        #print(title_ngrams)\n",
    "    return(bigram_queries)\n",
    "\n",
    "bigram_queries=get_bigrams(preprocessed_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ordered_bigram_matches(text, bigram):\n",
    "    \"\"\"Counts the number of bigram matches in text. Both text and bigram are represented as a list of terms.\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] == bigram[0]:\n",
    "            if text[i + 1] == bigram[1]:\n",
    "                count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_unordered_bigram_matches(text, bigram, w):\n",
    "    \"\"\"Counts the number of unordered bigram matches in text within a given window size. \n",
    "    Both text and bigram are represented as a list of terms.\"\"\"\n",
    "    count = 0\n",
    "    for i in range(len(text) - 1):\n",
    "        if text[i] in bigram:\n",
    "            \n",
    "            other_term = bigram[0] if text[i] == bigram[1] else bigram[1]\n",
    "            if other_term in text[i+1:i+w]:\n",
    "                count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_term_sequence(es, doc_id, field):\n",
    "    tv = es.termvectors(index=index_term, id=doc_id, fields=[field])\n",
    "    # We first put terms in a position-indexed dict.\n",
    "    pos = {}\n",
    "    for term, tinfo in tv['term_vectors'][field]['terms'].items():\n",
    "        for token in tinfo['tokens']:\n",
    "            pos[token['position']] = term\n",
    "    # Then, turn that dict to a list.\n",
    "    seq = [None] * (max(pos.keys()) + 1)\n",
    "    for p, term in pos.items():\n",
    "        seq[p] = term\n",
    "    return seq\n",
    "\n",
    "\n",
    "def get_bigram_counts(bigram_queries,method,field):\n",
    "\n",
    "    bigram_count={}\n",
    "    for q_id, query in bigram_queries.items():\n",
    "#         clear_output()\n",
    "        print(\"\\n--------------------------------------------------------------------\")\n",
    "        print(q_id, query)\n",
    "        bigrams={}\n",
    "        for q in query:\n",
    "            docs={}\n",
    "    #         print(\"\\n--------------------------------------------------------------------\")\n",
    "            q=\" \".join(q)\n",
    "            res = es.search(index=index_term, body={'query': {'match': {field: q}}})\n",
    "    #         pprint(res)\n",
    "            docs={}\n",
    "            for hit in res['hits']['hits']:\n",
    "    #             pprint(hit)\n",
    "                doc_id = hit['_id']\n",
    "                text = get_term_sequence(es, doc_id, field)\n",
    "    #             print(text)\n",
    "                if method==\"ordered\":\n",
    "                    count = count_ordered_bigram_matches(text, q.split())\n",
    "                else:\n",
    "                    count = count_unordered_bigram_matches(text, q.split(),4)\n",
    "\n",
    "\n",
    "                docs[doc_id]=count\n",
    "            bigrams[q]= docs\n",
    "        bigram_count[q_id]=bigrams\n",
    "        clear_output()\n",
    "    return(bigram_count)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_bigram_count=get_bigram_counts(bigram_queries, method=\"ordered\", field=\"content\")\n",
    "#order_bigram_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unorder_bigram_count=get_bigram_counts(bigram_queries,method=\"unordered\", field=\"content\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigram_scores(bigram_dictionary,q_id,bigram,doc_id,mu,field):\n",
    "    \n",
    "    try:\n",
    "        sum_doclen, doc_lens = length_doc_bigram(bigram_dictionary,q_id,bigram,field=field)\n",
    "        print(doc_lens)\n",
    "        P_bigram=sum(bigram_dictionary[q_id][bigram].values())/sum_doclen\n",
    "        return (math.log((bigram_dictionary[q_id][bigram][doc_id]+mu *P_bigram)/(mu +doc_lens[doc_id])))\n",
    "    except:\n",
    "        return 0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def length_doc_bigram (bigram_dictionary,q_id,bigram,field):\n",
    "    doc_lens = {}\n",
    "    \n",
    "    documents=bigram_dictionary[q_id][bigram].keys()\n",
    "    sum_doclen=0\n",
    "    for doc_id in documents:\n",
    "        print(doc_id)\n",
    "\n",
    "        doc_lens[doc_id]=0\n",
    "        if bigram_dictionary[q_id][bigram][doc_id]>0:\n",
    "            res = es.search(\n",
    "                index = index_term, \n",
    "                body = {\n",
    "                    'from': 0,\n",
    "                    'size': 1, # Maximum number of results to return\n",
    "                    \"query\": {\n",
    "                        \"match\": {\n",
    "                            \"_id\": doc_id\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            doc_lens[doc_id] = len(res['hits']['hits'][0]['_source'][field].split())\n",
    "\n",
    "        sum_doclen += doc_lens[doc_id]\n",
    "\n",
    "        \n",
    "        print(doc_lens[doc_id])\n",
    "    return (sum_doclen, doc_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculating f_o \n",
    "\n",
    "def rerank_(bigram_dictionary, field = 'content'):\n",
    "    doc_ids_={}\n",
    "    for q_id,bigram_dict in bigram_dictionary.items():\n",
    "        doc_ids=[]\n",
    "        for bigram,doc_dict in bigram_dict.items():\n",
    "            doc_ids = doc_ids + list(doc_dict.keys())\n",
    "        doc_ids_[q_id] = list(set(doc_ids_.get(q_id, []) + doc_ids))\n",
    "    # doc_ids_\n",
    "\n",
    "    score_q_doc={}\n",
    "    for q_id,bigram_dict in bigram_dictionary.items():\n",
    "        score_doc={}\n",
    "        for doc_id in doc_ids_[q_id]:\n",
    "            score=0\n",
    "            #print(type(doc_id))\n",
    "            for bigram in bigram_dict.keys():\n",
    "                clear_output()\n",
    "                score+=get_bigram_scores(bigram_dictionary,q_id,bigram,doc_id,mu=2000,field=field)\n",
    "            score_doc[doc_id]=score\n",
    "        score_q_doc[q_id]=score_doc\n",
    "    return(score_q_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_bigram_score=rerank_(order_bigram_count, field = 'content')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the score for unordered bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_order_bigram_score=rerank_(unorder_bigram_count, field = 'content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the scores query,document from different methods\n",
    "def Merge(dict1, dict2): \n",
    "#     res = {**dict1, **dict2} \n",
    "    for key, value in dict2.items():\n",
    "        if key in dict1.keys():\n",
    "            dict1[key] = dict1[key] + dict2[key]\n",
    "        else:\n",
    "            dict1[key] = dict2[key]\n",
    "    return dict1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SDM_unigram_scores=>unigrams\n",
    "#order_bigram_score=>ordered bigrams\n",
    "#un_order_bigram_score=>unordered bigrams\n",
    "\n",
    "SDM_score={}\n",
    "for q_id,query in preprocessed_q.items():\n",
    "    #print(q_id)\n",
    "    SDM_unigram_scores[q_id].update((x, y*0.8/len(preprocessed_q[q_id])) for x, y in SDM_unigram_scores[q_id].items())\n",
    "    order_bigram_score[q_id].update((x, y*0.05/(len(preprocessed_q[q_id])-1)) for x, y in order_bigram_score[q_id].items())\n",
    "    un_order_bigram_score[q_id].update((x, y*0.05/(len(preprocessed_q[q_id])-1)) for x, y in un_order_bigram_score[q_id].items())\n",
    "    merged_dict1=Merge(SDM_unigram_scores[q_id], order_bigram_score[q_id])\n",
    "    merged_dict_final=Merge(merged_dict1, un_order_bigram_score[q_id])\n",
    "    SDM_score[q_id]=merged_dict_final\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sdm_scores, score_df = sort_scores(preprocessed_q, SDM_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_csv(\"ranking_sdm_q2.csv\", index = False, sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing ELR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading query entities\n",
    "import json\n",
    "\n",
    "def load_entity_q(filepath):\n",
    "    with open(filepath) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        json_file.close()\n",
    "\n",
    "    return(data)\n",
    "data=load_entity_q('data/entity_annotations.json')\n",
    "#print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def score_ELR(entity,doc_id,weights=[0.2,0.8],lmbda=0.1):\n",
    "    entity = entity.lower()\n",
    "    fields=[\"label\",\"content\"]\n",
    "    \n",
    "    sum_score_field = 0\n",
    "    for i, weight in enumerate(weights):\n",
    "        tfe=0\n",
    "        dfe = 0\n",
    "        field=fields[i]\n",
    "        tv = es.mtermvectors(\n",
    "            index=index_entity, \n",
    "            doc_type='_doc',\n",
    "            body=dict(ids=[doc_id],parameters=dict(term_statistics=True,field_statistics=True,fields=[field])))\n",
    "        \n",
    "        \n",
    "        \n",
    "        pprint(tv['docs'][0]['term_vectors'].keys())\n",
    "        \n",
    "        field_len = 0\n",
    "        \n",
    "        if field not in tv['docs'][0]['term_vectors'].keys():\n",
    "            score_field = 0\n",
    "        else:\n",
    "            for terms in list(tv['docs'][0]['term_vectors'][field]['terms'].keys()):\n",
    "                field_len +=tv['docs'][0]['term_vectors'][field]['terms'][terms]['term_freq']\n",
    "\n",
    "            if entity in list(tv['docs'][0]['term_vectors'][field]['terms'].keys()):\n",
    "                tfe=1\n",
    "                dfe += 1\n",
    "\n",
    "            score_field = weight *((1-lmbda)*tfe +lmbda*(dfe/field_len))\n",
    "        sum_score_field+=score_field\n",
    "#         print(tfe,dfe, field_len)\n",
    "    return math.log(sum_score_field+1)\n",
    "\n",
    "    \n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(text):\n",
    "    text_words = []\n",
    "    for term in text.replace(\"<\",\"\").replace(\">\",\"\").split(\"_\"):\n",
    "        if term not in stop_list:\n",
    "            text_words.append(porter.stem(term))\n",
    "    \n",
    "    return \"_\".join(text_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerank_ELR(data):\n",
    "    score_query={}\n",
    "    for q_id,entities in data.items():\n",
    "        clear_output()\n",
    "        \n",
    "        all_entity_scores=[]\n",
    "\n",
    "        for entity,values in entities.items():\n",
    "            entity_=preprocess(entity)\n",
    "            res = es.search(index=index_entity, body={\n",
    "                'from': 0,\n",
    "                'size': 10, \n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"fields\": [\"label\", \"content\"],\n",
    "                        \"query\": entity_\n",
    "                    }\n",
    "                }\n",
    "            })\n",
    "            clear_output()\n",
    "            print(\"\\n\", entity_)\n",
    "            docs={}\n",
    "            for hit in res['hits']['hits']:\n",
    "                doc_id = hit['_id']\n",
    "                #print(doc_id)\n",
    "                score_doc=data[q_id][entity]['score']*score_ELR(entity_,doc_id)\n",
    "                docs[doc_id] = score_doc\n",
    "\n",
    "            if len(docs) > 0:\n",
    "                all_entity_scores.append(docs)\n",
    "        score_query[q_id]=all_entity_scores\n",
    "    # pprint(score_query)\n",
    "    \n",
    "    elr_score = {}\n",
    "    x={}\n",
    "    for q_id,scores in score_query.items():\n",
    "        for score in scores:\n",
    "            x= Merge(x,score)\n",
    "        elr_score[q_id]=dict(x)\n",
    "    clear_output()\n",
    "    return(elr_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elr_score=rerank_ELR(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final score SDR+ELR\n",
    "\n",
    "SDM_ELR_score={}\n",
    "for q_id,query in preprocessed_q.items():\n",
    "    #print(q_id)\n",
    "    weight_entity=0\n",
    "    for entity in data[q_id].keys():\n",
    "        weight_entity+=data[q_id][entity][\"score\"]\n",
    " \n",
    "    elr_score[q_id].update((x, y*0.1/weight_entity) for x, y in elr_score[q_id].items())\n",
    "    merged_dict=Merge(elr_score[q_id],SDM_score[q_id])\n",
    "    SDM_ELR_score[q_id]=merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_sdm_elr_scores, score_df = sort_scores(preprocessed_q, SDM_ELR_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_csv(\"ranking_sdm_elr_q2.csv\", index = False, sep = \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLM + ELR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLM_ELR_score={}\n",
    "for q_id,query in preprocessed_q.items():\n",
    "    #print(q_id)\n",
    "    weight_entity=0\n",
    "    for entity in data[q_id].keys():\n",
    "        weight_entity += data[q_id][entity][\"score\"]\n",
    " \n",
    "    elr_score[q_id].update((x, y*0.1/weight_entity) for x, y in elr_score[q_id].items())\n",
    "    \n",
    "    merged_dict=Merge(elr_score[q_id],mlm_scores[q_id])\n",
    "    \n",
    "    MLM_ELR_score[q_id]=merged_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_MLM_ELR_scores, score_df = sort_scores(preprocessed_q, MLM_ELR_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_csv(\"ranking_mlm_elr_q2.csv\", index = False, sep = \",\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
