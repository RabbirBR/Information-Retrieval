{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2B: Feature computation\n",
    "\n",
    "The purpose of this notebook is to perform the computation of features. \n",
    "\n",
    "Note that some features might be expensive, so you don't want to keep re-computing them. Instead, aim for writing a set of relatively simple feature extractors, each computing one or multiple features, and save their output to separate files. Then, load the pre-computed features from multiple files in the learning step (in the [ranking notebook](2_Ranking.ipynb))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extractors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example feature extractors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature computation\n",
    "\n",
    "Computes features for document-query pairs and saves them to a file.\n",
    "\n",
    "Specifically, we will save features to a JSON file, using a nested map structure, with queries on the first level, documents on the second level, and individual features on the third level. \n",
    "\n",
    "```python\n",
    "  features = {\n",
    "      'query_i': {\n",
    "          'doc_j': {\n",
    "              'feature_1': 0,  # value of feature_1 for (query_i, doc_j) pair\n",
    "              'feature_2': 0,  # value of feature_2 for (query_i, doc_j) pair\n",
    "              ...\n",
    "          }\n",
    "          ...\n",
    "      }\n",
    "      ...\n",
    "  }\n",
    "```\n",
    "\n",
    "**Note**: The set of documents for a query (for which you want to compute features) should be a combination of the documents for which you have relevance labels and the top-100 documents retrieved in first-pass retrieval.\n",
    "You can then decide in the learning part if/how you want to deal with class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import requests\n",
    "import json\n",
    "import math\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from IPython.display import clear_output # Using IPython.display.clear_output to clear the output of a cell.\n",
    "\n",
    "API = \"http://gustav1.ux.uis.no:5002\"\n",
    "\n",
    "MAIN_INDEX = \"clueweb12b\"\n",
    "ANCHORS_INDEX = \"clueweb12b_anchors\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries\n",
    "\n",
    "# Functions for the API.\n",
    "def tokenize_query(indexname, query):\n",
    "    url = \"/\".join([API, indexname, \"_analyze\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"text\": query})\n",
    "    response = requests.get(url).text\n",
    "    try:\n",
    "        r = json.loads(response)\n",
    "        return [t[\"token\"] for t in r[\"tokens\"]]\n",
    "    except:\n",
    "        print(\"Error in analyze query: \\n\", response)\n",
    "        return query.split()\n",
    "\n",
    "def search(indexname, query, field, size=10):\n",
    "    url = \"/\".join([API, indexname, \"_search\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"q\": query, \"df\": field, \"size\": size})\n",
    "    response = requests.get(url).text\n",
    "    \n",
    "    return json.loads(response)\n",
    "\n",
    "def exists(indexname, doc_id):\n",
    "    url = \"/\".join([API, indexname, doc_id, \"_exists\"])\n",
    "    response = requests.get(url).text\n",
    "    return json.loads(response)['exists']\n",
    "\n",
    "def analyze_query(indexname, query):\n",
    "    url = \"/\".join([API, indexname, \"_analyze\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"text\": query})\n",
    "    response = requests.get(url).text\n",
    "    r = json.loads(response)\n",
    "    return [t[\"token\"] for t in r[\"tokens\"]]\n",
    "\n",
    "def term_vectors(indexname, doc_id, term_statistics=False):\n",
    "    ret = {}    \n",
    "    url = \"/\".join([API, indexname, doc_id, \"_termvectors\"]) + \"?\" \\\n",
    "          + urllib.parse.urlencode({\"term_statistics\": str(term_statistics).lower()})\n",
    "    response = requests.get(url).text\n",
    "    try:\n",
    "        ret = json.loads(response)\n",
    "    except:\n",
    "        print(\"Failed to json-decode this response:\\n{}\".format(response))\n",
    "        \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_bm25(tokens, doc, field, index):\n",
    "    \"\"\"Feature: BM25 retrieval score on a given field.\"\"\"\n",
    "#     print('BM25:', tokens, doc, field, index)\n",
    "    k1 = 1.2\n",
    "    b  = 0.75\n",
    "\n",
    "    score = 0\n",
    "    try:\n",
    "        term_vector = term_vectors(index, doc, term_statistics=True)['term_vectors'][field]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "#     term_vector = term_vectors(index, doc, term_statistics=True)\n",
    "#     if 'term_vectors' not in term_vector.keys() or field not in term_vectors(index, doc, term_statistics=True)['term_vectors']:\n",
    "#         return 0\n",
    "    \n",
    "#     term_vector = term_vector['term_vectors'][field]\n",
    "    \n",
    "    avgdl = term_vector['field_statistics']['sum_ttf']/term_vector['field_statistics']['doc_count']\n",
    "    doc_len = sum([stats['term_freq'] for term, stats in term_vector['terms'].items()])\n",
    "\n",
    "    for term in tokens:\n",
    "        if term in term_vector['terms'].keys():\n",
    "            idf = math.log(term_vector['field_statistics']['doc_count']/term_vector['terms'][term]['doc_freq'])\n",
    "            f_td = term_vector['terms'][term]['term_freq']\n",
    "\n",
    "            term_score = idf*((f_td*(k1+1))/(f_td*(1-b+b*(doc_len/avgdl))))\n",
    "            \n",
    "            score = score + term_score\n",
    "            \n",
    "    return score\n",
    "    \n",
    "\n",
    "def feature_lm(tokens, doc, field, index):\n",
    "    \"\"\"Feature: LM retrieval score on a given field.\"\"\"\n",
    "#     print('LM:', tokens, doc, field, index)\n",
    "    lmbda = 0.8\n",
    "    score = 0\n",
    "    \n",
    "    try:\n",
    "        term_vector = term_vectors(index, doc, term_statistics=True)['term_vectors'][field]\n",
    "    except:\n",
    "        return 0\n",
    "    \n",
    "#     term_vector = term_vectors(index, doc, term_statistics=True)    \n",
    "#     if 'term_vectors' not in term_vector.keys() or field not in term_vectors(index, doc, term_statistics=True)['term_vectors']:\n",
    "#         return 0    \n",
    "#     term_vector = term_vector['term_vectors'][field]\n",
    "    \n",
    "    doc_len = sum([stats['term_freq'] for term, stats in term_vector['terms'].items()])\n",
    "\n",
    "    for term in tokens:\n",
    "        f_tq = tokens.count(term)\n",
    "        term_score = 0\n",
    "        \n",
    "        if term in term_vector['terms'].keys():\n",
    "            p_td = term_vector['terms'][term]['term_freq'] / doc_len\n",
    "            p_tC = term_vector['terms'][term]['ttf'] / term_vector['field_statistics']['sum_ttf']\n",
    "            \n",
    "            term_score = math.log(((1-lmbda)*p_td) + (lmbda*p_tC) + 1) * f_tq\n",
    "            \n",
    "        score = score + term_score\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pagerank = pd.read_csv(\"data/pagerank.docNameOrder\", sep = \" \", names = ['doc_id', 'rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the feature dictionary\n",
    "def get_features(queries):\n",
    "    \n",
    "    fields = [\"content\", \"title\", \"anchors\"]\n",
    "    features = {}\n",
    "    \n",
    "    for q, query in queries.items():\n",
    "        features[q] = {}\n",
    "\n",
    "        print(\"Working with {} - {}\".format(q, query))    \n",
    "\n",
    "        tokens = tokenize_query(MAIN_INDEX, query)\n",
    "        print(\"Query to API: \", ' '.join(tokens))\n",
    "\n",
    "    #   Build a list of documents from all the fields\n",
    "        print(\"Making list of docs\", end = \" - \")\n",
    "        \n",
    "        docs = []\n",
    "        for field in fields:\n",
    "            \n",
    "            index = MAIN_INDEX\n",
    "            if(field == 'anchors'):\n",
    "                index = ANCHORS_INDEX\n",
    "                \n",
    "            try:\n",
    "                search_res = search(index, ' '.join(tokens), field, size=100)['hits']['hits']\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            for doc in search_res:\n",
    "                if exists(MAIN_INDEX, doc['_id']) and exists(ANCHORS_INDEX, doc['_id']):\n",
    "                    docs.append(doc['_id'])\n",
    "             \n",
    "            print(field, 'done', end = \" - \")\n",
    "\n",
    "        docs = list(set(docs))\n",
    "        print(\"\\nNo. of documents:\", len(docs))\n",
    "        \n",
    "        i = 0\n",
    "        percent_done = 0\n",
    "        for d in docs:            \n",
    "            features[q][d] = {}\n",
    "\n",
    "#           Query Features\n",
    "            features[q][d]['q_len'] = len(query.split())\n",
    "            features[q][d]['q_token_len'] = len(tokens)\n",
    "        \n",
    "        \n",
    "\n",
    "#           Query Document Features\n",
    "            for field in fields:\n",
    "                index = MAIN_INDEX\n",
    "                if(field == 'anchors'):\n",
    "                    index = ANCHORS_INDEX\n",
    "                \n",
    "                features[q][d]['bm25_' + field] = feature_bm25(tokens, d, field, index)\n",
    "                features[q][d]['lm_'+ field] = feature_lm(tokens, d, field, index)\n",
    "                \n",
    "                \n",
    "                \n",
    "#           Document Features\n",
    "#           PageRank\n",
    "            features[q][d]['doc_pagerank'] = pagerank[pagerank['doc_id'] == d].iloc[0]['rank']\n",
    "            \n",
    "            url = \"http://gustav1.ux.uis.no:5002/\"+MAIN_INDEX+\"/\"+d+\"/_get\"\n",
    "            response = requests.get(url).text\n",
    "            response = json.loads(response)\n",
    "            try:\n",
    "                features[q][d]['doc_main_indx_length'] = response['_source']['length']\n",
    "            except:\n",
    "                features[q][d]['doc_main_indx_length'] = 0\n",
    "                \n",
    "                \n",
    "            url = \"http://gustav1.ux.uis.no:5002/\"+ANCHORS_INDEX+\"/\"+d+\"/_get\"\n",
    "            response = requests.get(url).text\n",
    "            response = json.loads(response)\n",
    "            try:\n",
    "                features[q][d]['doc_anchors_indx_length'] = len(list(response['_source'].values())[0].split())\n",
    "            except:\n",
    "                features[q][d]['doc_anchors_indx_length'] = 0\n",
    "                \n",
    "                \n",
    "#             print(features[q][d]['doc_main_indx_length'], features[q][d]['doc_anchors_indx_length'])\n",
    "    \n",
    "#           Showing percentage done, each '-' represents 1%\n",
    "            i = i + 1\n",
    "            if((i/len(docs))*100 >= (percent_done + 1)):\n",
    "                percent_done = percent_done + 1\n",
    "                print(\"-\",end=\"\")\n",
    "                \n",
    "                \n",
    "#         pprint(features[q])\n",
    "#         print(\"-------------------------------------------------------------------\")\n",
    "        print(\"\\n\")\n",
    "    clear_output()\n",
    "    print(\"Features Collected.\")\n",
    "    return features;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Collected.\n"
     ]
    }
   ],
   "source": [
    "train_features = get_features(load_queries(\"data/queries.txt\"))\n",
    "pickle.dump(train_features, open(\"data/train_features.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features Collected.\n"
     ]
    }
   ],
   "source": [
    "test_features = get_features(load_queries(\"data/queries2.txt\"))\n",
    "pickle.dump(test_features, open(\"data/test_features.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
