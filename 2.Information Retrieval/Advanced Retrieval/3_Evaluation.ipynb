{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2B: Evalution\n",
    "\n",
    "This notebook contains the skeleton for evaluating a document ranking against the ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading ranking file\n",
    "\n",
    "The file format is [specified in the assignment](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2b#output-file-format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANKING_FILE = \"data/ranking_bm25.csv\"\n",
    "\n",
    "ranking_csv = pd.read_csv(RANKING_FILE)\n",
    "\n",
    "qids = ranking_csv['QueryId'].unique()\n",
    "rankings = {}\n",
    "\n",
    "for Id in qids:    \n",
    "    r = ranking_csv[ranking_csv['QueryId'] == Id]['DocumentId'].to_list()    \n",
    "    rankings[str(Id)] = r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading relevance judgments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "gtruth = pickle.load(open(\"data/qrels.p\", \"rb\" )) # Built in Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing NDCG scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg(rel, p):\n",
    "    dcg = rel[0]\n",
    "    for i in range(1, min(p, len(rel))): \n",
    "        dcg += rel[i] / math.log(i + 1, 2)  # rank position is indexed from 1..\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def get_ndcg_scores(gtruth, rankings):\n",
    "    sum_ndcg5 = 0\n",
    "    sum_ndcg10 = 0\n",
    "    sum_ndcg20 = 0\n",
    "    sum_ndcg100 = 0\n",
    "\n",
    "    for qid, ranking in sorted(rankings.items()):\n",
    "        gt = gtruth[qid]\n",
    "        print(\"Query\", qid)\n",
    "\n",
    "        gains = [] # holds corresponding relevance levels for the ranked docs\n",
    "        for doc_id in ranking: \n",
    "            gain = gt.get(doc_id, 0)\n",
    "            gains.append(gain)\n",
    "        print(\"\\tGains:\", gains)\n",
    "\n",
    "        # relevance levels of the idealized ranking\n",
    "        gain_ideal = sorted([v for _, v in gt.items()], reverse=True)\n",
    "        print(\"\\tIdeal gains:\", gain_ideal)\n",
    "\n",
    "        ndcg5 = dcg(gains, 5) / dcg(gain_ideal, 5)\n",
    "        ndcg10 = dcg(gains, 10) / dcg(gain_ideal, 10)\n",
    "        ndcg20 = dcg(gains, 20) / dcg(gain_ideal, 20)\n",
    "        ndcg100 = dcg(gains, 100) / dcg(gain_ideal, 100)\n",
    "\n",
    "        sum_ndcg5 += ndcg5\n",
    "        sum_ndcg10 += ndcg10\n",
    "        sum_ndcg20 += ndcg20\n",
    "        sum_ndcg100 += ndcg100\n",
    "\n",
    "        print(\"\\tNDCG@5:\", round(ndcg5, 3), \"\\n\\tNDCG@10:\", round(ndcg10, 3), \"\\n\\tNDCG@20:\", round(ndcg20, 3), \"\\n\\tNDCG@100:\", round(ndcg100, 3))\n",
    "        print(\"----------------------------------------------------------------------------------------\")\n",
    "\n",
    "    clear_output()\n",
    "    print(\"Average\")\n",
    "    print(\"\\tNDCG@5:\", sum_ndcg5 / len(rankings), \"\\n\\tNDCG@10:\", sum_ndcg10 / len(rankings), \"\\n\\tNDCG@20:\", sum_ndcg20 / len(rankings), \"\\n\\tNDCG@100:\", sum_ndcg100 / len(rankings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "\tNDCG@5: 0.1779689659560673 \n",
      "\tNDCG@10: 0.16570814178359747 \n",
      "\tNDCG@20: 0.15341632649418818 \n",
      "\tNDCG@100: 0.11104042798957728\n"
     ]
    }
   ],
   "source": [
    "get_ndcg_scores(gtruth, rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features_and_labels(filepath):\n",
    "    \n",
    "    query_doc_relations = pickle.load(open(\"data/qrels.p\", \"rb\" )) # Computed in 'Preprocessing.ipynb' file\n",
    "    \n",
    "    features_dict = pickle.load(open(filepath, \"rb\" ))\n",
    "    \n",
    "    X = pd.DataFrame(columns=[\n",
    "        'q_id', \n",
    "        'doc_id', \n",
    "        'bm25_anchors', \n",
    "        'bm25_content', \n",
    "        'bm25_title', \n",
    "        'lm_anchors', \n",
    "        'lm_content', \n",
    "        'lm_title', \n",
    "        'q_len', \n",
    "        'q_token_len', \n",
    "        'doc_pagerank', \n",
    "        'doc_main_indx_length', \n",
    "        'doc_anchors_indx_length'\n",
    "    ])\n",
    "    \n",
    "    y = []\n",
    "    i = 0\n",
    "    for q_id, docs in features_dict.items():\n",
    "        for doc_id, features in docs.items():\n",
    "            clear_output()\n",
    "            print(\"For {} - {}\".format(q_id, doc_id))\n",
    "            X = X.append(pd.DataFrame(\n",
    "                [[q_id, doc_id, features['bm25_anchors'], features['bm25_content'], features['bm25_title'], features['lm_anchors'], features['lm_content'], features['lm_title'], features['q_len'],features['q_token_len'], features['doc_pagerank'], features['doc_main_indx_length'], features['doc_anchors_indx_length']]], \n",
    "                columns=[\n",
    "                    'q_id', \n",
    "                    'doc_id', \n",
    "                    'bm25_anchors', \n",
    "                    'bm25_content', \n",
    "                    'bm25_title', \n",
    "                    'lm_anchors', \n",
    "                    'lm_content', \n",
    "                    'lm_title', \n",
    "                    'q_len', \n",
    "                    'q_token_len', \n",
    "                    'doc_pagerank', \n",
    "                    'doc_main_indx_length', \n",
    "                    'doc_anchors_indx_length'\n",
    "                ]))\n",
    "            \n",
    "            if doc_id in query_doc_relations[q_id]:\n",
    "                y.append(query_doc_relations[q_id][doc_id])\n",
    "            else:\n",
    "                y.append(0)\n",
    "            \n",
    "            print(X.iloc[i])\n",
    "            print(y[i])\n",
    "            i += 1\n",
    "    \n",
    "    X = X.reset_index(drop=True)\n",
    "    clear_output()\n",
    "    return (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_test_features(filepath):\n",
    "    \n",
    "    query_doc_relations = pickle.load(open(\"data/qrels.p\", \"rb\" )) # Computed in 'Preprocessing.ipynb' file\n",
    "    \n",
    "    features_dict = pickle.load(open(filepath, \"rb\" ))\n",
    "    \n",
    "    X = pd.DataFrame(columns=[\n",
    "        'q_id', \n",
    "        'doc_id', \n",
    "        'bm25_anchors', \n",
    "        'bm25_content', \n",
    "        'bm25_title', \n",
    "        'lm_anchors', \n",
    "        'lm_content', \n",
    "        'lm_title', \n",
    "        'q_len', \n",
    "        'q_token_len', \n",
    "        'doc_pagerank', \n",
    "        'doc_main_indx_length', \n",
    "        'doc_anchors_indx_length'\n",
    "    ])\n",
    "    \n",
    "    i = 0\n",
    "    for q_id, docs in features_dict.items():\n",
    "        for doc_id, features in docs.items():\n",
    "            clear_output()\n",
    "            print(\"For {} - {}\".format(q_id, doc_id))\n",
    "            X = X.append(pd.DataFrame(\n",
    "                [[q_id, doc_id, features['bm25_anchors'], features['bm25_content'], features['bm25_title'], features['lm_anchors'], features['lm_content'], features['lm_title'], features['q_len'],features['q_token_len'], features['doc_pagerank'], features['doc_main_indx_length'], features['doc_anchors_indx_length']]], \n",
    "                columns=[\n",
    "                    'q_id', \n",
    "                    'doc_id', \n",
    "                    'bm25_anchors', \n",
    "                    'bm25_content', \n",
    "                    'bm25_title', \n",
    "                    'lm_anchors', \n",
    "                    'lm_content', \n",
    "                    'lm_title', \n",
    "                    'q_len', \n",
    "                    'q_token_len', \n",
    "                    'doc_pagerank', \n",
    "                    'doc_main_indx_length', \n",
    "                    'doc_anchors_indx_length'\n",
    "                ]))\n",
    "            \n",
    "            print(X.iloc[i])\n",
    "            i += 1\n",
    "    X = X.reset_index(drop=True)\n",
    "    clear_output()\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_features_and_labels(filepath = \"data/train_features.p\")\n",
    "test_X = load_test_features(filepath = \"data/test_features.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = preprocessing.LabelEncoder()\n",
    "Encoder.fit(pd.concat([X['doc_id'], test_X['doc_id']], axis=0))\n",
    "\n",
    "X['doc_id'] = Encoder.transform(X['doc_id'])\n",
    "test_X['doc_id'] = Encoder.transform(test_X['doc_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rankings(test_X, Regressor, ranking_file_name):\n",
    "    rankings = pd.DataFrame(columns=[\n",
    "            'q_id',\n",
    "            'doc_id',\n",
    "            'relevance'\n",
    "    ])\n",
    "    \n",
    "    rankings_dict = {}\n",
    "\n",
    "    queries = list(test_X['q_id'].unique())\n",
    "\n",
    "    for q_id in queries:\n",
    "        rankings_dict[q_id] = []\n",
    "        \n",
    "        test_X_qid = test_X[test_X['q_id'] == q_id]\n",
    "\n",
    "        predictions = Regressor.predict(test_X_qid)\n",
    "        test_X_qid = test_X_qid.assign(relevance=predictions)\n",
    "\n",
    "        test_X_qid = test_X_qid.sort_values(by=['relevance'], ascending = False).head(100).reset_index()\n",
    "        \n",
    "        rankings = pd.concat([rankings, test_X_qid[['q_id', 'doc_id', 'relevance']]], ignore_index=True)        \n",
    "        rankings_dict[q_id] = list(Encoder.inverse_transform(test_X_qid['doc_id']))\n",
    "\n",
    "    rankings['doc_id'] = rankings['doc_id'].astype(int)\n",
    "    rankings['doc_id'] = Encoder.inverse_transform(rankings['doc_id'])\n",
    "\n",
    "    rankings.rename(columns={\"q_id\": \"QueryId\", \"doc_id\": \"DocumentId\"}, inplace = True)\n",
    "    rankings[['QueryId', 'DocumentId']].to_csv(ranking_file_name, index = False)\n",
    "    \n",
    "    clear_output()\n",
    "    return rankings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query-Document Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "\tNDCG@5: 0.044138193945731284 \n",
      "\tNDCG@10: 0.046388364114609885 \n",
      "\tNDCG@20: 0.043307932502692895 \n",
      "\tNDCG@100: 0.025958321382731006\n"
     ]
    }
   ],
   "source": [
    "Regressor = DecisionTreeRegressor(max_depth=2, min_samples_split=200).fit(X_train[['q_id', 'doc_id', 'bm25_anchors', 'bm25_content', 'bm25_title','lm_anchors','lm_content','lm_title']], y_train)\n",
    "\n",
    "rankings = get_rankings(X_val[['q_id', 'doc_id', 'bm25_anchors', 'bm25_content', 'bm25_title','lm_anchors','lm_content','lm_title']], Regressor, ranking_file_name = 'ranking_QD.csv')\n",
    "\n",
    "\n",
    "get_ndcg_scores(gtruth, rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query-Document + Query Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "\tNDCG@5: 0.044138193945731284 \n",
      "\tNDCG@10: 0.046388364114609885 \n",
      "\tNDCG@20: 0.043307932502692895 \n",
      "\tNDCG@100: 0.025958321382731006\n"
     ]
    }
   ],
   "source": [
    "Regressor = DecisionTreeRegressor(max_depth=2, min_samples_split=200).fit(X_train[['q_id', 'doc_id', 'bm25_anchors', 'bm25_content', 'bm25_title','lm_anchors','lm_content','lm_title', 'q_len', 'q_token_len']], y_train)\n",
    "\n",
    "rankings = get_rankings(X_val[['q_id', 'doc_id', 'bm25_anchors', 'bm25_content', 'bm25_title','lm_anchors','lm_content','lm_title', 'q_len', 'q_token_len']], Regressor, ranking_file_name = 'ranking_QD_Q.csv')\n",
    "\n",
    "get_ndcg_scores(gtruth, rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query-Document + Document Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "\tNDCG@5: 0.044138193945731284 \n",
      "\tNDCG@10: 0.046388364114609885 \n",
      "\tNDCG@20: 0.043307932502692895 \n",
      "\tNDCG@100: 0.025958321382731006\n"
     ]
    }
   ],
   "source": [
    "Regressor = DecisionTreeRegressor(max_depth=2, min_samples_split=200).fit(X_train[['q_id', 'doc_id', 'bm25_anchors', 'bm25_content', 'bm25_title','lm_anchors','lm_content','lm_title', 'doc_pagerank', 'doc_main_indx_length', 'doc_anchors_indx_length']], y_train)\n",
    "\n",
    "rankings = get_rankings(X_val[['q_id', 'doc_id', 'bm25_anchors', 'bm25_content', 'bm25_title','lm_anchors','lm_content','lm_title', 'doc_pagerank', 'doc_main_indx_length', 'doc_anchors_indx_length']], Regressor, ranking_file_name = 'ranking_QD_D.csv')\n",
    "\n",
    "get_ndcg_scores(gtruth, rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query-Document Features + Query + Document Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "\tNDCG@5: 0.044138193945731284 \n",
      "\tNDCG@10: 0.046388364114609885 \n",
      "\tNDCG@20: 0.043307932502692895 \n",
      "\tNDCG@100: 0.025958321382731006\n"
     ]
    }
   ],
   "source": [
    "Regressor = DecisionTreeRegressor(max_depth=2, min_samples_split=200).fit(X_train, y_train)\n",
    "\n",
    "rankings = get_rankings(X_val, Regressor, ranking_file_name = 'ranking_all.csv')\n",
    "\n",
    "get_ndcg_scores(gtruth, rankings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Best Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bm25_anchors',\n",
       " 'bm25_content',\n",
       " 'bm25_title',\n",
       " 'doc_main_indx_length',\n",
       " 'doc_anchors_indx_length']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "feature_names = ['bm25_anchors', 'bm25_content', 'bm25_title','lm_anchors','lm_content','lm_title','q_len', 'q_token_len','doc_pagerank','doc_main_indx_length','doc_anchors_indx_length']\n",
    "\n",
    "k_best = SelectKBest(chi2, k=5).fit(X[feature_names], y)\n",
    "\n",
    "best_features = []\n",
    "\n",
    "mask = k_best.get_support()\n",
    "\n",
    "for bool, feature in zip(mask, feature_names):\n",
    "    if bool:\n",
    "        best_features.append(feature)\n",
    "        \n",
    "best_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "\tNDCG@5: 0.044138193945731284 \n",
      "\tNDCG@10: 0.046388364114609885 \n",
      "\tNDCG@20: 0.043307932502692895 \n",
      "\tNDCG@100: 0.025958321382731006\n"
     ]
    }
   ],
   "source": [
    "Regressor = DecisionTreeRegressor(max_depth=2, min_samples_split=200).fit(X_train, y_train)\n",
    "\n",
    "rankings = get_rankings(X_val, Regressor, ranking_file_name = 'ranking_all_test.csv')\n",
    "\n",
    "get_ndcg_scores(gtruth, rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average\n",
      "\tNDCG@5: 0.033444050971747394 \n",
      "\tNDCG@10: 0.038253602483916245 \n",
      "\tNDCG@20: 0.04039123782635726 \n",
      "\tNDCG@100: 0.02421182661920442\n"
     ]
    }
   ],
   "source": [
    "Regressor = DecisionTreeRegressor(max_depth=2, min_samples_split=200).fit(X_train[['q_id', 'doc_id'] + best_features], y_train)\n",
    "\n",
    "rankings = get_rankings(X_val[['q_id', 'doc_id'] + best_features], Regressor, ranking_file_name = 'ranking_best_featuers_test.csv')\n",
    "\n",
    "get_ndcg_scores(gtruth, rankings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
