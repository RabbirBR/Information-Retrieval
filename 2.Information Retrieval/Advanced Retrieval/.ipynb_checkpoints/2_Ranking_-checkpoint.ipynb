{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2B: Ranking\n",
    "\n",
    "This notebook contains the skeleton for training a model and then applying it to produce a document ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the precomputed features\n",
    "\n",
    "The code below loads the precomputed features and combines them into feature vectors for query-document pairs.\n",
    "\n",
    "For this part to work, you'll need to run the `1_Feature_computation` notebook first to generate the sample features JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qrels = {}\n",
    "\n",
    "with open(\"data/qrels.csv\") as f:    \n",
    "    for line in f:        \n",
    "        line = line.rstrip().split(',')\n",
    "        if line[0] == 'QueryId':\n",
    "            continue        \n",
    "        \n",
    "        if line[0] not in qrels.keys():\n",
    "            qrels[line[0]] = {\n",
    "                line[1]: int(line[2])\n",
    "            }\n",
    "        else:\n",
    "            qrels[line[0]][line[1]] = int(line[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = pd.read_csv('data/train_features.csv')\n",
    "train_y = []\n",
    "\n",
    "for row in train_X.iterrows():\n",
    "    train_y.append(qrels[row['q_id']][row['doc_id']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = pd.read_csv('data/test_features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "Encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "Encoder.fit(pd.concat([train_X['doc_id'], test_X['doc_id']], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X['doc_id'] = Encoder.transform(train_X['doc_id'])\n",
    "test_X['doc_id'] = Encoder.transform(test_X['doc_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## Training a model -->\n",
    "\n",
    "Training needs to be done differently based on the scenario:\n",
    "\n",
    "  * **Scenario 1**: The model is trained using cross-validation, that is on 4/5 of queries, then applied on the remaining 1/5 of queries (repeated 5 times).\n",
    "  * **Scenario 2**: The model is trained on all available training data.\n",
    "  \n",
    "The feature vectors at this point are already created. These should contain both (a) the training queries and (b) the queries on which you want to apply your model.\n",
    "\n",
    "Train your model on queries (a). For that you'll also need to load the corresponding relevance labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taken from 'https://www.kaggle.com/davidgasquez/ndcg-scorer'\n",
    "\n",
    "\"\"\"Metrics to compute the model performance.\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "\n",
    "def dcg_score(y_true, y_score, k=5):\n",
    "    \"\"\"Discounted cumulative gain (DCG) at rank K.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array, shape = [n_samples]\n",
    "        Ground truth (true relevance labels).\n",
    "    y_score : array, shape = [n_samples, n_classes]\n",
    "        Predicted scores.\n",
    "    k : int\n",
    "        Rank.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "    \"\"\"\n",
    "    order = np.argsort(y_score)[::-1]\n",
    "    y_true = np.take(y_true, order[:k])\n",
    "\n",
    "    gain = 2 ** y_true - 1\n",
    "\n",
    "    discounts = np.log2(np.arange(len(y_true)) + 2)\n",
    "    return np.sum(gain / discounts)\n",
    "\n",
    "\n",
    "def ndcg_score(ground_truth, predictions, k=5):\n",
    "    \"\"\"Normalized discounted cumulative gain (NDCG) at rank K.\n",
    "\n",
    "    Normalized Discounted Cumulative Gain (NDCG) measures the performance of a\n",
    "    recommendation system based on the graded relevance of the recommended\n",
    "    entities. It varies from 0.0 to 1.0, with 1.0 representing the ideal\n",
    "    ranking of the entities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    ground_truth : array, shape = [n_samples]\n",
    "        Ground truth (true labels represended as integers).\n",
    "    predictions : array, shape = [n_samples, n_classes]\n",
    "        Predicted probabilities.\n",
    "    k : int\n",
    "        Rank.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    score : float\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    >>> ground_truth = [1, 0, 2]\n",
    "    >>> predictions = [[0.15, 0.55, 0.2], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n",
    "    >>> score = ndcg_score(ground_truth, predictions, k=2)\n",
    "    1.0\n",
    "    >>> predictions = [[0.9, 0.5, 0.8], [0.7, 0.2, 0.1], [0.06, 0.04, 0.9]]\n",
    "    >>> score = ndcg_score(ground_truth, predictions, k=2)\n",
    "    0.6666666666\n",
    "    \"\"\"\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(len(predictions) + 1))\n",
    "    T = lb.transform(ground_truth)\n",
    "\n",
    "    scores = []\n",
    "\n",
    "    # Iterate over each y_true and compute the DCG score\n",
    "    for y_true, y_score in zip(T, predictions):\n",
    "        actual = dcg_score(y_true, y_score, k)\n",
    "        best = dcg_score(y_true, y_true, k)\n",
    "        try:\n",
    "            score = float(actual) / float(best)\n",
    "        except:\n",
    "            score = 0\n",
    "        scores.append(score)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# NDCG Scorer function\n",
    "ndcg_scorer = make_scorer(ndcg_score, needs_proba=False, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "parameters = {\n",
    "    'max_depth': [depth for depth in range(2, 20, 2)],\n",
    "    'min_samples_split': [min_samples for min_samples in range(200, 2001, 200)]\n",
    "}\n",
    "\n",
    "gridSearchResult = GridSearchCV(DecisionTreeRegressor(random_state = 0),\n",
    "                                parameters, \n",
    "                                cv=5, \n",
    "                                scoring=ndcg_scorer, \n",
    "                                verbose=5\n",
    "                               ).fit(train_X, train_y)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame.from_dict(gridSearchResult.cv_results_)\n",
    "results_df.sort_values(by=['rank_test_score']).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Regressor = gridSearchResult.best_estimator_\n",
    "Regressor.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the model to produce a ranking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the train model on queries (b) and sort documents according to the predicted relevance score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "Regressor.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = pd.DataFrame(columns=[\n",
    "        'q_id',\n",
    "        'doc_id',\n",
    "        'relevance'\n",
    "])\n",
    "\n",
    "queries = list(test_X['q_id'].unique())\n",
    "\n",
    "for q_id in queries:\n",
    "    test_X_qid = test_X[test_X['q_id'] == q_id]\n",
    "    \n",
    "    predictions = Regressor.predict(test_X_qid)\n",
    "    test_X_qid = test_X_qid.assign(relevance=predictions)\n",
    "    \n",
    "    test_X_qid = test_X_qid.sort_values(by=['relevance'], ascending = False).head(100).reset_index()\n",
    "    rankings = pd.concat([rankings, test_X_qid[['q_id', 'doc_id', 'relevance']]], ignore_index=True)\n",
    "    \n",
    "rankings['doc_id'] = rankings['doc_id'].astype(int)\n",
    "rankings['doc_id'] = Encoder.inverse_transform(rankings['doc_id'])\n",
    "\n",
    "rankings.rename(columns={\"q_id\": \"QueryId\", \"doc_id\": \"DocumentId\"}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings.to_csv('own_ranking.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings[['QueryId', 'DocumentId']].to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
