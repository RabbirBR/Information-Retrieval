{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A, Part 1: Indexer\n",
    "\n",
    "Index the document collection and save the index to disk.  \n",
    "\n",
    "**IMPORTANT**: The collection and index take up several hundred Megabytes. Do NOT push those to GitHub!\n",
    "\n",
    "It is recommended that you work on a small sample of documents while developing your solution. It is enough to build the full index once you get to Part 2 of the assignment, as you may realize later that certain refinements are needed.\n",
    "\n",
    "You have two main options to implement the inverted index: (1) all by yourself from scratch or (2) using the [HashedIndex](https://pypi.org/project/hashedindex/) Python library. There is no third option.\n",
    "\n",
    "You are required to adhere to the structure provided below.\n",
    "\n",
    "The code for parsing the gzip files in the collection is already given.\n",
    "\n",
    "You may decide to build two separate indices for the two document fields (title and content) or to keep them together in the same structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import hashedindex\n",
    "from hashedindex import textparser\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "\n",
    "from IPython.display import clear_output # Using IPython.display.clear_output to clear the output of a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_docs_bulk(docs, section):\n",
    "    indexes = hashedindex.HashedIndex()\n",
    "    doclen = {}\n",
    "    tC = {}\n",
    "    total_term_count = 0\n",
    "    \n",
    "    for doc_id, doc in docs.items():\n",
    "        # TODO: complete\n",
    "#         print(\"Indexing document {}\".format(doc_id))\n",
    "        \n",
    "        terms = list(textparser.word_tokenize((doc[section]).lower(), stopwords = nltk_stopwords))\n",
    "        doclen[doc_id] = len(terms)\n",
    "        \n",
    "        for term in terms:\n",
    "            indexes.add_term_occurrence(term[0], doc_id)\n",
    "        \n",
    "    indexes = indexes.items()\n",
    "    for term, doc_freq_pair in indexes.items():\n",
    "        doc_freq_pair = dict(doc_freq_pair)\n",
    "        indexes[term] = doc_freq_pair\n",
    "        term_count = sum(list(doc_freq_pair.values()))\n",
    "        \n",
    "        tC[term] = term_count\n",
    "        total_term_count += term_count\n",
    "        \n",
    "    return (indexes, doclen, tC, total_term_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing a given data file\n",
    "\n",
    "**NOTE**: Each source gzip file contains several documents. The method below does the parsing of source files and then calls `add_docs_bulk()` to bulk indexing on all document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': {'1': 100, '2': 100, '5': 100, '3': 100},\n",
       " 'b': {'1': 200, '3': 100},\n",
       " 'c': {'1': 200, '3': 100}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def combine_indexes(prev_indexes, new_indexes):\n",
    "    for k, val in new_indexes.items():\n",
    "        if k in prev_indexes.keys():\n",
    "            prev_indexes[k].update(new_indexes[k])\n",
    "        else:\n",
    "            prev_indexes[k] = new_indexes[k]\n",
    "    \n",
    "    return prev_indexes\n",
    "\n",
    "# Simple example of this function\n",
    "example_dict_1 = {'a': {'1':100, '2': 100}, 'b': {'1':200, '3': 100}}\n",
    "example_dict_2 = {'a': {'5':100, '3': 100}, 'c': {'1':200, '3': 100}}\n",
    "\n",
    "combine_indexes(example_dict_1, example_dict_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_file(file_names, section):\n",
    "    doc_len = {}\n",
    "    indexes = {}\n",
    "    P_tc = {}\n",
    "    total_term_count = 0\n",
    "    \n",
    "    total_files_indexed = 0\n",
    "    gz_files_read = 0\n",
    "    for file_name in file_names:\n",
    "        gz_files_read += 1\n",
    "        clear_output()\n",
    "        print(\"Processing\", file_name)\n",
    "        docs = {}\n",
    "        with gzip.open(file_name, \"rt\") as fin:\n",
    "            is_body = False\n",
    "            doc_id, body = None, None\n",
    "            \n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"<DOCNO>\"):  # get doc id\n",
    "                    doc_id = re.sub(\"<DOCNO> | </DOCNO>\", \"\", line)\n",
    "                elif line.startswith(\"<BODY>\"):  # start to parse body\n",
    "                    is_body = True\n",
    "                    body = []\n",
    "                elif line.startswith(\"</BODY>\"):  # finished reading body\n",
    "                    soup = BeautifulSoup(\"\\n\".join(body), \"lxml\")\n",
    "                    headline = soup.find(\"headline\")\n",
    "                    text = soup.find(\"text\")\n",
    "                    docs[doc_id] = {\n",
    "                        \"title\": headline.text if headline is not None else \"\",  # use an empty string if no <HEADLINE> found\n",
    "                        \"content\": text.text if text is not None else \"\"  # everything inside <TEXT> is indexed as content\n",
    "                    }\n",
    "                    # get ready for next document\n",
    "                    doc_id = None\n",
    "                    is_body = False\n",
    "                elif is_body:  # accumulate body content\n",
    "                    body.append(line)\n",
    "\n",
    "            # bulk index the collected documents\n",
    "            total_files_indexed += len(docs)\n",
    "            print(\"Bulk indexed:\", len(docs), \"documents.\")\n",
    "            print(\"Total files indexed so far:\", total_files_indexed)\n",
    "            print(gz_files_read,\"/\",len(file_names), \"gz files finished reading.\")\n",
    "            new_indexes, doclen, tC, total_tc = add_docs_bulk(docs, section)\n",
    "            \n",
    "            # Concatanate and combine the indexes\n",
    "            indexes = combine_indexes(indexes, new_indexes)\n",
    "            \n",
    "            # Concatenate the new document lengths\n",
    "            doc_len.update(doclen)\n",
    "            \n",
    "            # Sum all term counts\n",
    "            for term, count in tC.items():\n",
    "                if term in P_tc.keys():\n",
    "                    P_tc[term] = P_tc[term] + count\n",
    "                else:\n",
    "                    P_tc[term] = count\n",
    "                    \n",
    "            # Add the total term count\n",
    "            total_term_count += total_tc\n",
    "    \n",
    "    # Calculate P(t|C) needed for Language Model\n",
    "    for term, count in P_tc.items():\n",
    "        P_tc[term] = P_tc[term]/total_term_count\n",
    "        \n",
    "    clear_output()\n",
    "    print(\"Finished indexing\", total_files_indexed, \"files in\", len(file_names), \"gz files.\")\n",
    "    return (indexes, doc_len, P_tc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing for only one collection for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished indexing 243 files in 1 gz files.\n",
      "16379 243 16379\n"
     ]
    }
   ],
   "source": [
    "indexes, doc_len, P_tC = index_file(glob.glob(\"data/aquaint/nyt/2000/20000101_NYT.gz\"), section = 'content')\n",
    "print(len(indexes), len(doc_len), len(P_tC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Save the index to disk (make sure that the index directory is added to `.gitignore`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the entire collection and writing it to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished indexing 1033461 files in 3344 gz files.\n",
      "824277 1033461 824277\n"
     ]
    }
   ],
   "source": [
    "# Indexing all files for the content\n",
    "indexes, doc_len, P_tC = index_file(glob.glob(\"data/aquaint/**/*.gz\", recursive=True), section = 'content')\n",
    "print(len(indexes), len(doc_len), len(P_tC))\n",
    "\n",
    "# Writing the data to appropriate files\n",
    "pickle.dump(indexes, open(\"data/indexes_content.p\", \"wb\"))\n",
    "pickle.dump(doc_len, open(\"data/doc_len_content.p\", \"wb\")) # Needed for BM25 and LM (Jelinek-Mercer smoothing)\n",
    "pickle.dump(P_tC, open(\"data/P_tC_content.p\", \"wb\")) # Needed for LM (Jelinek-Mercer smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished indexing 1033461 files in 3344 gz files.\n",
      "79448 1033461 79448\n"
     ]
    }
   ],
   "source": [
    "# Indexing all files for the title\n",
    "indexes, doc_len, P_tC = index_file(glob.glob(\"data/aquaint/**/*.gz\", recursive=True), section = 'title')\n",
    "print(len(indexes), len(doc_len), len(P_tC))\n",
    "\n",
    "# Writing the data to appropriate files\n",
    "pickle.dump(indexes, open(\"data/indexes_title.p\", \"wb\"))\n",
    "pickle.dump(doc_len, open(\"data/doc_len_title.p\", \"wb\")) # Needed for BM25 and LM (Jelinek-Mercer smoothing)\n",
    "pickle.dump(P_tC, open(\"data/P_tC_title.p\", \"wb\")) # Needed for LM (Jelinek-Mercer smoothing) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
