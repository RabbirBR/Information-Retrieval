{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A, Part 2: Retrieval\n",
    "\n",
    "Implement BM25 and LM retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/queries.txt\"  # make sure the query file exists on this location\n",
    "OUTPUT_FILE = \"data/output.txt\"  # output the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import hashedindex\n",
    "from hashedindex import textparser\n",
    "import glob\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from IPython.display import clear_output # Using IPython.display.clear_output to clear the output of a cell.\n",
    "\n",
    "nltk_stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: place the indexing related code here. This may be copy-pasted from Part 1.\n",
    "def add_docs_bulk(docs, section):\n",
    "    indexes = hashedindex.HashedIndex()\n",
    "    doclen = {}\n",
    "    tC = {}\n",
    "    total_term_count = 0\n",
    "    \n",
    "    for doc_id, doc in docs.items():\n",
    "        # TODO: complete\n",
    "#         print(\"Indexing document {}\".format(doc_id))\n",
    "        \n",
    "        terms = list(textparser.word_tokenize((doc[section]).lower(), stopwords = nltk_stopwords))\n",
    "        doclen[doc_id] = len(terms)\n",
    "        \n",
    "        for term in terms:\n",
    "            indexes.add_term_occurrence(term[0], doc_id)\n",
    "        \n",
    "    indexes = indexes.items()\n",
    "    for term, doc_freq_pair in indexes.items():\n",
    "        doc_freq_pair = dict(doc_freq_pair)\n",
    "        indexes[term] = doc_freq_pair\n",
    "        term_count = sum(list(doc_freq_pair.values()))\n",
    "        \n",
    "        tC[term] = term_count\n",
    "        total_term_count += term_count\n",
    "        \n",
    "    return (indexes, doclen, tC, total_term_count)\n",
    "\n",
    "\n",
    "def combine_indexes(prev_indexes, new_indexes):\n",
    "    for k, val in new_indexes.items():\n",
    "        if k in prev_indexes.keys():\n",
    "            prev_indexes[k].update(new_indexes[k])\n",
    "        else:\n",
    "            prev_indexes[k] = new_indexes[k]\n",
    "    \n",
    "    return prev_indexes\n",
    "\n",
    "\n",
    "def index_file(file_names, section):\n",
    "    doc_len = {}\n",
    "    indexes = {}\n",
    "    P_tc = {}\n",
    "    total_term_count = 0\n",
    "    \n",
    "    total_files_indexed = 0\n",
    "    gz_files_read = 0\n",
    "    for file_name in file_names:\n",
    "        gz_files_read += 1\n",
    "        clear_output()\n",
    "        print(\"Processing\", file_name)\n",
    "        docs = {}\n",
    "        with gzip.open(file_name, \"rt\") as fin:\n",
    "            is_body = False\n",
    "            doc_id, body = None, None\n",
    "            \n",
    "            for line in fin:\n",
    "                line = line.strip()\n",
    "                if line.startswith(\"<DOCNO>\"):  # get doc id\n",
    "                    doc_id = re.sub(\"<DOCNO> | </DOCNO>\", \"\", line)\n",
    "                elif line.startswith(\"<BODY>\"):  # start to parse body\n",
    "                    is_body = True\n",
    "                    body = []\n",
    "                elif line.startswith(\"</BODY>\"):  # finished reading body\n",
    "                    soup = BeautifulSoup(\"\\n\".join(body), \"lxml\")\n",
    "                    headline = soup.find(\"headline\")\n",
    "                    text = soup.find(\"text\")\n",
    "                    docs[doc_id] = {\n",
    "                        \"title\": headline.text if headline is not None else \"\",  # use an empty string if no <HEADLINE> found\n",
    "                        \"content\": text.text if text is not None else \"\"  # everything inside <TEXT> is indexed as content\n",
    "                    }\n",
    "                    # get ready for next document\n",
    "                    doc_id = None\n",
    "                    is_body = False\n",
    "                elif is_body:  # accumulate body content\n",
    "                    body.append(line)\n",
    "\n",
    "            # bulk index the collected documents\n",
    "            total_files_indexed += len(docs)\n",
    "            print(\"Bulk indexed:\", len(docs), \"documents.\")\n",
    "            print(\"Total files indexed so far:\", total_files_indexed)\n",
    "            print(gz_files_read,\"/\",len(file_names), \"gz files finished reading.\")\n",
    "            new_indexes, doclen, tC, total_tc = add_docs_bulk(docs, section)\n",
    "            \n",
    "            # Concatanate and combine the indexes\n",
    "            indexes = combine_indexes(indexes, new_indexes)\n",
    "            \n",
    "            # Concatenate the new document lengths\n",
    "            doc_len.update(doclen)\n",
    "            \n",
    "            # Sum all term counts\n",
    "            for term, count in tC.items():\n",
    "                if term in P_tc.keys():\n",
    "                    P_tc[term] = P_tc[term] + count\n",
    "                else:\n",
    "                    P_tc[term] = count\n",
    "                    \n",
    "            # Add the total term count\n",
    "            total_term_count += total_tc\n",
    "    \n",
    "    # Calculate P(t|C) needed for Language Model\n",
    "    for term, count in P_tc.items():\n",
    "        P_tc[term] = P_tc[term]/total_term_count\n",
    "        \n",
    "    clear_output()\n",
    "    print(\"Finished indexing\", total_files_indexed, \"files in\", len(file_names), \"gz files.\")\n",
    "    return (indexes, doc_len, P_tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Indexes and doc_len\n",
    "# indexes, doc_len, P_tC = index_file(glob.glob(\"data/aquaint/**/*.gz\", recursive=True), section = 'content')\n",
    "# print(len(indexes), len(doc_len), len(P_tC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "I have already calculated the indexes, the document length and the P(t|C) in '1_Indexer' so I am only loading that data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indexes, doc_len and P(t|C) for content data\n",
    "inv_idx = pickle.load(open(\"data/indexes_content.p\", \"rb\" ))\n",
    "doc_len = pickle.load(open(\"data/doc_len_content.p\", \"rb\" ))\n",
    "P_tC = pickle.load(open(\"data/P_tC_content.p\", \"rb\" ))\n",
    "NUM_DOCS = len(doc_len)\n",
    "avg_dl = sum(list(doc_len.values()))/NUM_DOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the queries from the file\n",
    "\n",
    "See the assignment description for the format of the query file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "queries = load_queries(QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write your scoring code here\n",
    "def get_BM25_score_for(q_id, query, k1, b):    \n",
    "    print(\"Ranking documents using BM25 for [%s] '%s'\" % (q_id, query))\n",
    "    scores = {}\n",
    "    q_words = query.lower().replace(\"-\", \" \").replace(\",\", \"\").split()\n",
    "    \n",
    "    # TODO generate ranking\n",
    "    for term in list(set(q_words)):\n",
    "        if term in list(inv_idx.keys()):\n",
    "            for (doc_id, f_td) in inv_idx[term].items():\n",
    "                n = len(inv_idx[term])\n",
    "                idf = math.log((NUM_DOCS - n + 0.5) / (n + 0.5))\n",
    "                \n",
    "                score_for_term =  round((idf*(f_td * (k1 + 1)) / (f_td + k1*(1 - b + b*(doc_len[doc_id]/avg_dl)))), 3)\n",
    "                \n",
    "                scores[doc_id] = scores.get(doc_id, 0) + score_for_term\n",
    "                \n",
    "    scores = sorted(scores.items(), key=lambda score: score[1], reverse = True)[:100]\n",
    "    \n",
    "    return scores\n",
    "\n",
    "def get_LM_score_for(q_id, query, lmbda):\n",
    "    print(\"Ranking documents using Jelinek-Mercer smoothing for [%s] '%s'\" % (q_id, query))\n",
    "    \n",
    "    scores = {}\n",
    "    q_words = query.lower().replace(\"-\", \" \").replace(\",\", \"\").split()\n",
    "    \n",
    "    for term in list(set(q_words)):\n",
    "        f_tq = q_words.count(term)\n",
    "        \n",
    "        if term in list(inv_idx.keys()):\n",
    "            for (doc_id, f_td) in inv_idx[term].items():\n",
    "                score_for_term =  round(math.log((1-lmbda)*(f_td / doc_len[doc_id]) + (lmbda*P_tC[term])), 3)\n",
    "    \n",
    "                scores[doc_id] = scores.get(doc_id, 0) + score_for_term\n",
    "            scores[doc_id] = f_tq * scores[doc_id]\n",
    "    \n",
    "    scores = sorted(scores.items(), key=lambda score: score[1], reverse = True)[:100]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Generate a ranking for each query and output the results to `OUTPUT_FILE`\n",
    "\n",
    "See the assignment description for the format of the output file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "BM25_data = pd.DataFrame(columns=['QueryId', 'DocumentId'])\n",
    "LM_data = pd.DataFrame(columns=['QueryId', 'DocumentId'])\n",
    "\n",
    "for q_id, query in queries.items():\n",
    "    # TODO generate ranking\n",
    "    BM_25_rankings = get_BM25_score_for(q_id, query, k1 = 1.2, b = 0.7)\n",
    "    LM_rankings = get_LM_score_for(q_id, query, lmbda = 0.8)\n",
    "    \n",
    "    \n",
    "    # TODO write results to file\n",
    "    for score in BM_25_rankings:\n",
    "        BM25_data = BM25_data.append(pd.DataFrame([[q_id, score[0]]], columns=['QueryId', 'DocumentId']))\n",
    "        \n",
    "    for score in LM_rankings:\n",
    "        LM_data = LM_data.append(pd.DataFrame([[q_id, score[0]]], columns=['QueryId', 'DocumentId']))\n",
    "    \n",
    "    clear_output()\n",
    "\n",
    "# Save scores to files \n",
    "BM25_data.to_csv(\"bm25_singlefield.csv\", index = False)\n",
    "LM_data.to_csv(\"lm_singlefield.csv\", index = False)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Report on the evaluation results (using the [Evaluation notebook](1_Evaluation.ipynb)) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the parameter settings used for the two methods: \n",
    "\n",
    "Parameters for BM25: k1 = 1.2, b = 0.7\n",
    "Parameters for LM: lambda = 0.8\n",
    "\n",
    "Write the name of the corresponding output file in the table. These files should be pushed to your repository.\n",
    "\n",
    "| **Method** | **Output file** | **P@10** | **MAP** | **MRR** |\n",
    "| -- | -- | -- | -- | -- |\n",
    "| BM25 | `bm25_singlefield.csv` | 0.172 | 0.063 | 0.292 |\n",
    "| LM | `lm_singlefield.csv` | 0.010 | 0.002 | 0.022 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
