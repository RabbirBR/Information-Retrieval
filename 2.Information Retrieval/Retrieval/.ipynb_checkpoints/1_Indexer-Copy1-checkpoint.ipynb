{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A, Part 1: Indexer\n",
    "\n",
    "Index the document collection and save the index to disk.  \n",
    "\n",
    "**IMPORTANT**: The collection and index take up several hundred Megabytes. Do NOT push those to GitHub!\n",
    "\n",
    "It is recommended that you work on a small sample of documents while developing your solution. It is enough to build the full index once you get to Part 2 of the assignment, as you may realize later that certain refinements are needed.\n",
    "\n",
    "You have two main options to implement the inverted index: (1) all by yourself from scratch or (2) using the [HashedIndex](https://pypi.org/project/hashedindex/) Python library. There is no third option.\n",
    "\n",
    "You are required to adhere to the structure provided below.\n",
    "\n",
    "The code for parsing the gzip files in the collection is already given.\n",
    "\n",
    "You may decide to build two separate indices for the two document fields (title and content) or to keep them together in the same structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import gzip\n",
    "from bs4 import BeautifulSoup\n",
    "import hashedindex\n",
    "from hashedindex import textparser\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import nltk\n",
    "nltk_stopwords = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "from IPython.display import clear_output # Using IPython.display.clear_output to clear the output of a cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_docs_bulk(docs, param_type):\n",
    "    indx = hashedindex.HashedIndex()\n",
    "    dlen = {}\n",
    "    tC = {}\n",
    "    term_count = 0\n",
    "    \n",
    "    for doc_id, doc in docs.items():        \n",
    "        terms = list(textparser.word_tokenize((doc[param_type]).lower(), stopwords = nltk_stopwords))\n",
    "        dlen[doc_id] = len(terms)\n",
    "        \n",
    "        for term in terms:\n",
    "            indx.add_term_occurrence(term[0], doc_id)\n",
    "        \n",
    "    indx = indx.items()\n",
    "    for term, doc_freq_pair in indx.items():\n",
    "        doc_freq_pair = dict(doc_freq_pair)\n",
    "        indx[term] = doc_freq_pair\n",
    "        term_count = sum(list(doc_freq_pair.values()))\n",
    "        \n",
    "        tC[term] = term_count\n",
    "        term_count += term_count\n",
    "        \n",
    "    return (indx, dlen, tC, term_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing a given data file\n",
    "\n",
    "**NOTE**: Each source gzip file contains several documents. The method below does the parsing of source files and then calls `add_docs_bulk()` to bulk indexing on all document "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_file(file_name, param_type):\n",
    "    docs = {}\n",
    "    with gzip.open(file_name, \"rt\") as fin:\n",
    "        print(\"Working with:\", file_name)\n",
    "        is_body = False\n",
    "        doc_id, body = None, None\n",
    "\n",
    "        for line in fin:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"<DOCNO>\"):  # get doc id\n",
    "                doc_id = re.sub(\"<DOCNO> | </DOCNO>\", \"\", line)\n",
    "            elif line.startswith(\"<BODY>\"):  # start to parse body\n",
    "                is_body = True\n",
    "                body = []\n",
    "            elif line.startswith(\"</BODY>\"):  # finished reading body\n",
    "                soup = BeautifulSoup(\"\\n\".join(body), \"lxml\")\n",
    "                headline = soup.find(\"headline\")\n",
    "                text = soup.find(\"text\")\n",
    "                docs[doc_id] = {\n",
    "                    \"title\": headline.text if headline is not None else \"\",  # use an empty string if no <HEADLINE> found\n",
    "                    \"content\": text.text if text is not None else \"\"  # everything inside <TEXT> is indexed as content\n",
    "                }\n",
    "                # get ready for next document\n",
    "                doc_id = None\n",
    "                is_body = False\n",
    "            elif is_body:  # accumulate body content\n",
    "                body.append(line)\n",
    "        clear_output()\n",
    "        return add_docs_bulk(docs, param_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing the all files and writing it to pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_files = glob.glob(\"data/aquaint/**/*.gz\", recursive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PtC' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f80d293199bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/content_indx.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0md_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/content_d_len.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPtC\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"data/content_PtC.p\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'PtC' is not defined"
     ]
    }
   ],
   "source": [
    "# For Content\n",
    "indx = {}\n",
    "d_len = {}\n",
    "Ptc = {}\n",
    "term_count = 0\n",
    "\n",
    "for file in all_files:\n",
    "    new_indx, dlen, tC, tc = index_file(file, 'content')\n",
    "    \n",
    "    for k, val in new_indx.items():\n",
    "        if k in indx.keys():\n",
    "            indx[k].update(new_indx[k])\n",
    "        else:\n",
    "            indx[k] = new_indx[k]\n",
    "    \n",
    "    d_len.update(dlen)\n",
    "    \n",
    "    for term, count in tC.items():\n",
    "        if term in Ptc.keys():\n",
    "            Ptc[term] = Ptc[term] + count\n",
    "        else:\n",
    "            Ptc[term] = count\n",
    "            \n",
    "    term_count += tc\n",
    "    \n",
    "for term, count in Ptc.items():\n",
    "    Ptc[term] = Ptc[term]/term_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(indx, open(\"data/content_indx.p\", \"wb\"))\n",
    "pickle.dump(d_len, open(\"data/content_d_len.p\", \"wb\"))\n",
    "pickle.dump(Ptc, open(\"data/content_PtC.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Title\n",
    "indx = {}\n",
    "d_len = {}\n",
    "Ptc = {}\n",
    "term_count = 0\n",
    "\n",
    "for file in all_files:\n",
    "    new_indx, dlen, tC, tc = index_file(file, 'title')\n",
    "    \n",
    "    for k, val in new_indx.items():\n",
    "        if k in indx.keys():\n",
    "            indx[k].update(new_indx[k])\n",
    "        else:\n",
    "            indx[k] = new_indx[k]\n",
    "    \n",
    "    d_len.update(dlen)\n",
    "    \n",
    "    for term, count in tC.items():\n",
    "        if term in Ptc.keys():\n",
    "            Ptc[term] = Ptc[term] + count\n",
    "        else:\n",
    "            Ptc[term] = count\n",
    "            \n",
    "    term_count += tc\n",
    "    \n",
    "for term, count in Ptc.items():\n",
    "    Ptc[term] = Ptc[term]/term_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(indx, open(\"data/title_indx.p\", \"wb\"))\n",
    "pickle.dump(d_len, open(\"data/title_d_len.p\", \"wb\"))\n",
    "pickle.dump(Ptc, open(\"data/title_PtC.p\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
