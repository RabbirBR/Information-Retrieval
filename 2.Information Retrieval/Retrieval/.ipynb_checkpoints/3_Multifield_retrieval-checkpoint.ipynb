{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2A, Part 3: Multifield retrieval\n",
    "\n",
    "Implement BM25F and the Mixture of Language Models (MLM). Use two fields: title and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_FILE = \"data/queries.txt\"  # make sure the query file exists on this location\n",
    "OUTPUT_FILE = \"data/output.txt\"  # output the ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "\n",
    "from IPython.display import clear_output # Using IPython.display.clear_output to clear the output of a cell.\n",
    "\n",
    "nltk_stopwords = set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "I have already calculated the indexes, the document length and the P(t|C) in '1_Indexer' so I am only loading that data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: place the indexing related code here. This may be copy-pasted from Part 1.\n",
    "inv_idx, doc_len, P_tC, avg_dl = [{}, {}], [{}, {}], [{}, {}], [0, 0] # one index for each parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load indexes, doc_len and P(t|C) for content data\n",
    "inv_idx[0] = pickle.load(open(\"data/indexes_content.p\", \"rb\" ))\n",
    "doc_len[0] = pickle.load(open(\"data/doc_len_content.p\", \"rb\" ))\n",
    "P_tC[0] = pickle.load(open(\"data/P_tC_content.p\", \"rb\" ))\n",
    "NUM_DOCS = len(doc_len[0])\n",
    "avg_dl[0] = sum(list(doc_len[0].values()))/NUM_DOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_idx[1] = pickle.load(open(\"data/indexes_title.p\", \"rb\" ))\n",
    "doc_len[1] = pickle.load(open(\"data/doc_len_title.p\", \"rb\" ))\n",
    "P_tC[1] = pickle.load(open(\"data/P_tC_title.p\", \"rb\" ))\n",
    "\n",
    "avg_dl[1] = sum(list(doc_len[1].values()))/NUM_DOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the queries from the file\n",
    "\n",
    "See the assignment description for the format of the query file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a#queries)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_queries(query_file):\n",
    "    queries = {}\n",
    "    with open(query_file, \"r\") as fin:\n",
    "        for line in fin.readlines():\n",
    "            qid, query = line.strip().split(\" \", 1)\n",
    "            queries[qid] = query\n",
    "    return queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "queries = load_queries(QUERY_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO write your scoring code here\n",
    "def get_BM25f_ranking_for(q_id, query, k1, b, weights):\n",
    "    scores = {}\n",
    "    \n",
    "    q_words = query.lower().replace(\"-\", \" \").replace(\",\", \"\").split()\n",
    "    \n",
    "    for term in list(set(q_words)):\n",
    "        for i, w_i in enumerate(weights):\n",
    "            if term in list(inv_idx[i].keys()):\n",
    "                # If the term is in the inverse index\n",
    "                _f_td = {}\n",
    "                \n",
    "                n = len(inv_idx[i][term])\n",
    "                idf = math.log((NUM_DOCS - n + 0.5) / (n + 0.5))\n",
    "                \n",
    "                for (doc_id, f_td) in inv_idx[i][term].items():                    \n",
    "                    B_i = 1 - b[i] + b[i]*(doc_len[i][doc_id]/avg_dl[i])\n",
    "                    \n",
    "                    _f_td[doc_id] = _f_td.get(doc_id, 0) + (w_i*(f_td/B_i))\n",
    "                    \n",
    "                    score_for_term = round(idf*(_f_td[doc_id]/(k1 + _f_td[doc_id])), 3)\n",
    "                    scores[doc_id] = scores.get(doc_id, 0) + score_for_term\n",
    "            \n",
    "    scores = sorted(scores.items(), key=lambda score: score[1], reverse = True)[:100]\n",
    "    return scores\n",
    "\n",
    "def get_MLM_ranking_for(q_id, query, lmbda, weights):    \n",
    "    scores = {}\n",
    "    q_words = query.lower().replace(\"-\", \" \").replace(\",\", \"\").split()\n",
    "    \n",
    "    for term in list(set(q_words)):\n",
    "        for i, w_i in enumerate(weights):\n",
    "            if (term in list(inv_idx[i].keys())):\n",
    "                # If the term is in the inverse index\n",
    "                for (doc_id, f_td) in inv_idx[i][term].items():\n",
    "                    score_for_term = w_i*((1-lmbda[i])*(f_td / doc_len[i][doc_id]) + (lmbda[i]*P_tC[i][term]))\n",
    "                    \n",
    "                    scores[doc_id] = scores.get(doc_id, 0) + score_for_term\n",
    "                \n",
    "    for doc_id, score in scores.items():\n",
    "        scores[doc_id] = math.log(scores[doc_id])\n",
    "    \n",
    "    scores = sorted(scores.items(), key=lambda score: score[1], reverse = True)[:100]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO** Generate a ranking for each query and output the results to `OUTPUT_FILE`\n",
    "\n",
    "See the assignment description for the format of the output file [here](https://github.com/kbalog/uis-dat640-fall2019/tree/master/assignments/assignment-2a#output-file-format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def BM25f_ranking_data(k1, b, weights):\n",
    "    BM25f_data = pd.DataFrame(columns=['QueryId', 'DocumentId'])\n",
    "    \n",
    "    for q_id, query in queries.items():\n",
    "        # TODO generate ranking\n",
    "#         print(q_id, query)\n",
    "        BM_25f_rankings = get_BM25f_ranking_for(q_id, query, k1 = k1, b = b, weights = weights)\n",
    "\n",
    "        for score in BM_25f_rankings:\n",
    "            BM25f_data = BM25f_data.append(pd.DataFrame([[q_id, score[0]]], columns=['QueryId', 'DocumentId']))\n",
    "\n",
    "    return BM25f_data\n",
    "    \n",
    "def MLM_ranking_data(lmbda, weights):\n",
    "    MLM_data = pd.DataFrame(columns=['QueryId', 'DocumentId'])\n",
    "    \n",
    "    for q_id, query in queries.items():\n",
    "        # TODO generate ranking\n",
    "#         print(q_id, query)\n",
    "        MLM_rankings = get_MLM_ranking_for(q_id, query, lmbda = lmbda, weights = weights)\n",
    "\n",
    "        # TODO write results to file\n",
    "        for score in MLM_rankings:\n",
    "            MLM_data = MLM_data.append(pd.DataFrame([[q_id, score[0]]], columns=['QueryId', 'DocumentId']))\n",
    "\n",
    "    return MLM_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25f_data = BM25f_ranking_data(k1 = 1, b = [0.2, 0.3], weights = [0.9, 0.1])\n",
    "MLM_data = MLM_ranking_data(lmbda = [0.8,0.8], weights = [0.9, 0.1])\n",
    "\n",
    "BM25f_data.to_csv(\"bm25f_multifield.csv\", index = False)\n",
    "MLM_data.to_csv(\"mlm_multifield.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different parameters for the \"Parameter Tuning\".\n",
    "\n",
    "# bm25f_params = {\n",
    "#     'k1': [x for x in np.arange(1.0, 2.1, 0.1)], # k1, calibrating term frequency scaling, can be any value over 0, for the experiment I'm using 1 <= k1 <= 2\n",
    "#     'b': [(x, y) for x in np.arange(0, 1.1, 0.2) for y in np.arange(0, 1.1, 0.2)], # b, the normalization factors, is a tuple of values in which either of the values can be 0 <= b <= 1\n",
    "#     'weights': [(x, 1-x) for x in np.arange(0, 1.1, 0.2)], # weights is a tuple of 2 values which is 1 when summed\n",
    "# }\n",
    "\n",
    "\n",
    "# mlm_params = {\n",
    "#     'lmbda': [(x, y) for x in np.arange(0.1, 1.1, 0.1) for y in np.arange(0.1, 1.1, 0.1)], # lambda, the smoothing parameter\n",
    "#     'weights': [(x, 1-x) for x in np.arange(0.1, 1.0, 0.1)], # weights is a tuple of 2 values which is 1 when summed\n",
    "# } "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "Report on the evaluation results (using the [Evaluation notebook](1_Evaluation.ipynb)) here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried the brute force approach and defined a range of values for each of the variables, but it takes a very long time. So, I tried some trial and error, but I didn't randomly select the parameters, I used a similar approach as the solution to the 12 marbles puzzle. I did not have enough time to code and let it run to do a comprehensive test of this random theory I just had, so I only did a few iterations manually. The idea is as follows -\n",
    "\n",
    "All my parameters are in a specific range and also in ascending order. \n",
    "\n",
    "First, I calculate accuracy of the 25th and 75th percentile of a parameter and also the accuracy of the median of the set of values for the parameter. Compared to the median, if the accuracy was higher on the 25th percentile as opposed to the 75th percentile I would redefine my set of parameters to all values in the first half of the set, otherwise I would redefine my set of parameters to all values in the second half of the set. Then I would repeat the process until either the index of the 25th percentile is higher or equal to the median or the index of the 75th percentile is lower or equal to the median, in any of these 2 cases I would simply return the median parameter combination. In any other case, I would return the parameter value with the higher accuracy.\n",
    "\n",
    "If I did this recursively for each parameters, this would significantly reduce the computation time. But for this I had a major assumption that the accuracy would increase or decrease if I simply selected a specific \"direction\" in each iteration in each parameter.\n",
    "\n",
    "\n",
    "| **Method** | **Parameter settings** | **Output file** | **P@10** | **MAP** | **MRR** |\n",
    "| -- | -- | -- | -- | -- | -- |\n",
    "| BM25F | k1: 11, b: [0.3, 0.25], $w_{title}$: 0.1, $w_{content}$: 0.9 | `data/bm25f_multifield.csv` | 0.174 | 0.066 | 0.282 |\n",
    "| MLM | Smoothing method: Jelinek-Mercer, smoothing param: lambda = [0.8,0.8], $w_{title}$: 0.1, $w_{content}$: 0.9 | `data/mlm_multifield.csv` | 0.130 | 0.046 | 0.221 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
